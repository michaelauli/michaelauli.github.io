exclude: ['README.md']
timezone: US/Pacific
talks:
  -
    title: Sequence to Sequence Learning&#58; Fast Training and Inference with Gated Convolutions
    detail: Talk at Johns Hopkins University, Oct 2017.
    url: "talks/convs2s.pdf"
    img: "convs2s.png"
  -
    title: Learning to translate with neural networks
    detail: Talk at Facebook, Google, Amazon and the University of Washington, 2014.
    url: "talks/facebook-rnn-translate.pdf"
    img: "facebook-rnn-translate.jpg"
  -
    title: Integrated Parsing and Tagging
    detail: Talk at Carnegie Mellon University, Johns Hopkins University, BBN Technologies, IBM Research and Microsoft Research, 2011.
    url: "talks/parsing-tagging-talk.pdf"
    img: "parsing-tagging-talk.jpg"
papers:
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    title: "ELI5: Long Form Question Answering"
    authors: Angela Fan, Yacine Jernite, Ethan Perez, Jason Weston, Michael Auli
    doc-url:
    img: inpress
    booktitle: Proc. of ACL
    booktitle-url:
    code:
    venue: conference
    abstract: >
      We introduce the first large-scale corpus for long-form question
      answering, a task requiring elaborate and in-depth answers to
      open-ended questions. The dataset comprises 270K threads from
      the Reddit forum ``Explain Like I'm Five'' (ELI5) where an
      online community provides answers to questions which are
      comprehensible by five year olds.  Compared to existing
      datasets, ELI5 comprises diverse questions requiring
      multi-sentence answers. We provide a large set of web documents
      to help answer the question. % in order to find support for the
      answer. Automatic and human evaluations show that an abstractive
      model trained with a multi-task objective outperforms
      conventional Seq2Seq, language modeling, as well as a strong
      extractive baseline.  However, our best model is still far from
      human performance since raters prefer gold responses in over
      86\% of cases, leaving ample opportunity for future improvement.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    title: "wav2vec: Unsupervised Pre-training for Speech Recognition"
    authors: Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli
    doc-url: https://arxiv.org/pdf/1904.05862
    img: wav2vec
    booktitle: arXiv
    booktitle-url: https://arxiv.org/abs/1904.05862
    code:
    venue: conference
    abstract: >
      We explore unsupervised pre-training for speech recognition by
      learning representations of raw audio. wav2vec is trained on
      large amounts of unlabeled audio data and the resulting
      representations are then used to improve acoustic model
      training. We pre-train a simple multi-layer convolutional neural
      network optimized via a noise contrastive binary classification
      task. Our experiments on WSJ reduce WER of a strong
      character-based log-mel filterbank baseline by up to 32% when
      only a few hours of transcribed data is available. Our approach
      achieves 2.43% WER on the nov92 test set. This outperforms Deep
      Speech 2, the best reported character-based system in the
      literature while using three orders of magnitude less labeled
      training data.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    title: "fairseq: A fast, extensible toolkit for sequence modeling"
    authors: Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli
    doc-url: https://arxiv.org/pdf/1904.01038
    img: fairseq
    booktitle: Proc. of NAACL, Demonstrations
    booktitle-url: https://arxiv.org/abs/1904.01038
    code: https://github.com/pytorch/fairseq
    venue: conference
    abstract: >
      fairseq is an open-source sequence modeling toolkit that
      allows researchers and developers to train custom models
      for translation, summarization, language modeling, and
      other text generation tasks. The toolkit is based on PyTorch
      and supports distributed training across multiple GPUs
      and machines. We also support fast mixed-precision training
      and inference on modern GPUs. A demo video can be found
      here: https://www.youtube.com/watch?v=OtgDdWtHvto
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    title: "Pre-trained Language Model Representations for Language Generation"
    authors: Sergey Edunov, Alexei Baevski, Michael Auli
    doc-url: https://arxiv.org/pdf/1903.09722
    img: pretrain_langgen
    booktitle: Proc. of NAACL
    booktitle-url: https://arxiv.org/abs/1903.09722
    code: https://github.com/pytorch/fairseq
    venue: conference
    abstract: >
      Pre-trained language model representations have been
      successful in a wide range of language understanding tasks.
      In this paper, we examine different strategies to integrate
      pretrained representations into sequence to sequence models
      and apply it to neural machine translation and abstractive
      summarization. We find that pre-trained representations are
      most effective when added to the encoder network which slows
      inference by only 14%. Our experiments in machine translation
      show gains of up to 5.3 BLEU in a simulated resource-poor setup.
      While returns diminish with more labeled data, we still
      observe improvements when millions of sentence-pairs are
      available. Finally, on abstractive summarization we achieve
      a new state of the art on the full text version of
      CNN-DailyMail.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    title: "Cloze-driven Pretraining of Self-attention Networks"
    authors: Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli
    doc-url: https://arxiv.org/pdf/1903.07785
    img: cloze
    booktitle: arXiv
    booktitle-url: https://arxiv.org/abs/1903.07785
    code:
    venue: conference
    abstract: >
      We present a new approach for pretraining a bi-directional
      transformer model that provides significant performance
      gains across a variety of language understanding problems.
      Our model solves a cloze-style word reconstruction task,
      where each word is ablated and must be predicted given the
      rest of the text. Experiments demonstrate large performance
      gains on GLUE and new state of the art results on NER as
      well as constituency parsing benchmarks, consistent with
      the concurrently introduced BERT model. We also present a
      detailed analysis of a number of factors that contribute
      to effective pretraining, including data domain and size,
      model capacity, and variations on the cloze objective.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    title: "Mixture Models for Diverse Machine Translation: Tricks of the Trade"
    authors: Tianxiao Shen, Myle Ott, Michael Auli, Marc'Aurelio Ranzato
    doc-url: https://arxiv.org/pdf/1902.07816
    img: moe
    booktitle: Proc. of ICML
    booktitle-url: https://arxiv.org/abs/1902.07816
    code: https://github.com/pytorch/fairseq
    venue: conference
    abstract: >
      Mixture models trained via EM are among the simplest, most
      widely used and well understood latent variable models in the
      machine learning literature. Surprisingly, these models have
      been hardly explored in text generation applications such as
      machine translation. In principle, they provide a latent
      variable to control generation and produce a diverse set of
      hypotheses. In practice, however, mixture models are prone to
      degeneracies---often only one component gets trained or the
      latent variable is simply ignored. We find that disabling
      dropout noise in responsibility computation is critical to
      successful training. In addition, the design choices of
      parameterization, prior distribution, hard versus soft EM and
      online versus offline assignment can dramatically affect model
      performance. We develop an evaluation protocol to assess both
      quality and diversity of generations against multiple
      references, and provide an extensive empirical study of several
      mixture model variants. Our analysis shows that certain types of
      mixture models are more robust and offer the best trade-off
      between translation quality and diversity compared to
      variational models and diverse decoding approaches.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    authors: Dario Pavllo, Christoph Feichtenhofer, Michael Auli, David Grangier
    title: Modeling Human Motion with Quaternion-based Neural Networks
    doc-url: https://arxiv.org/pdf/1901.07677
    img: quaternet
    booktitle: arXiv
    booktitle-url: https://arxiv.org/abs/1901.07677
    venue: conference
    abstract: >
      Previous work on predicting or generating 3D human pose
      sequences regresses either joint rotations or joint positions.
      The former strategy is prone to error accumulation along
      the kinematic chain, as well as discontinuities when using
      Euler angles or exponential maps as parameterizations.
      The latter requires re-projection onto skeleton constraints
      to avoid bone stretching and invalid configurations.
      This work addresses both limitations. QuaterNet represents
      rotations with quaternions and our loss function performs
      forward kinematics on a skeleton to penalize absolute
      position errors instead of angle errors.
      We investigate both recurrent and convolutional architectures
      and evaluate on short-term prediction and long-term generation.
      For the latter, our approach is qualitatively judged as
      realistic as recent neural strategies from the graphics
      literature.
      Our experiments compare quaternions to Euler angles
      as well as exponential maps and show that only a
      very short context is required to make reliable future
      predictions.
      Finally, we show that the standard evaluation protocol
      for Human3. 6M produces high variance results and
      we propose a simple solution.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    authors: Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, Michael Auli
    title: Pay Less Attention with Lightweight and Dynamic Convolutions
    doc-url: https://arxiv.org/pdf/1901.10430.pdf
    code: https://github.com/pytorch/fairseq
    img: dynamicconv
    booktitle: Proc. of ICLR
    booktitle-url: https://arxiv.org/abs/1901.10430
    venue: conference
    abstract: >
      Self-attention is a useful mechanism to build generative models
      for language and images.
      It determines the importance of context elements by comparing
      each element to the current time step. In this paper, we show
      that a very lightweight convolution can perform competitively
      to the best reported self-attention results.
      Next, we introduce dynamic convolutions which are simpler and
      more efficient than self-attention.
      We predict separate convolution kernels based solely on the
      current time-step in order to determine the importance of
      context elements.
      The number of operations required by this approach scales
      linearly in the input length, whereas self-attention is quadratic.
      Experiments on large-scale machine translation,
      language modeling and abstractive summarization show that
      dynamic convolutions improve over strong self-attention models.
      On the WMT'14 English-German test set dynamic convolutions
      achieve a new state of the art of 29.7 BLEU.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    authors: Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston
    title: 'Wizard of Wikipedia: Knowledge-Powered Conversational agents'
    doc-url: https://arxiv.org/pdf/1811.01241
    img: wizard
    booktitle: Proc. of ICLR
    booktitle-url: https://arxiv.org/abs/1811.01241
    venue: conference
    abstract: >
      In open-domain dialogue intelligent agents should exhibit
      the use of knowledge, however there are few convincing demonstrations
      of this to date.
      The most popular sequence to sequence models typically" generate
      and hope" generic utterances that can be memorized in the weights
      of the model when mapping from input utterance (s) to output,
      rather than employing recalled knowledge as context.
      Use of knowledge has so far proved difficult, in part because
      of the lack of a supervised learning benchmark task which exhibits
      knowledgeable open dialogue with clear grounding.
      To that end we collect and release a large dataset with conversations
      directly grounded with knowledge retrieved from Wikipedia.
      We then design architectures capable of retrieving knowledge,
      reading and conditioning on it, and finally generating natural
      responses.
      Our best performing dialogue models are able to conduct
      knowledgeable discussions on open-domain topics as evaluated
      by automatic metrics and human evaluations, while our
      new benchmark allows for measuring further improvements
      in this important research direction.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    authors: Alexei Baevski, Michael Auli
    title: Adaptive Input Representations for Neural Language Modeling
    doc-url: https://arxiv.org/pdf/1809.10853
    img: adaptive
    booktitle: Proc. of ICLR
    code: https://github.com/pytorch/fairseq
    booktitle-url: https://arxiv.org/abs/1809.10853
    venue: conference
    abstract: >
      We introduce adaptive input representations for neural language
      modeling which extend the adaptive softmax of
      Grave et al. (2017) to input representations of variable capacity.
      There are several choices on how to factorize the input and
      output layers, and whether to model words, characters or
      sub-word units.
      We perform a systematic comparison of popular choices for
      a self-attentional architecture. Our experiments show that
      models equipped with adaptive embeddings are more than
      twice as fast to train than the popular character input CNN
      while having a lower number of parameters.
      We achieve a new state of the art on the WikiText-103
      benchmark of 20.51 perplexity, improving the next best
      known result by 8.7 perplexity. On the Billion word benchmark,
      we achieve a state of the art of 24.14 perplexity.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2018
    authors: Dario Pavllo, Christoph Feichtenhofer, David Grangier, Michael Auli
    title: 3D human pose estimation in video with temporal convolutions and semi-supervised training
    doc-url: https://arxiv.org/pdf/1811.11742
    code: https://github.com/facebookresearch/VideoPose3D
    img: videopose
    booktitle: Proc. of CVPR
    booktitle-url: https://arxiv.org/abs/1811.11742
    venue: conference
    abstract: >
      In this work, we demonstrate that 3D poses in video can
      be effectively estimated with a fully convolutional model
      based on dilated temporal convolutions over 2D keypoints.
      We also introduce back-projection, a simple and effective
      semi-supervised training method that leverages unlabeled
      video data.
      We start with predicted 2D keypoints for unlabeled video,
      then estimate 3D poses and finally back-project to
      the input 2D keypoints.
      In the supervised setting, our fully-convolutional model
      outperforms the previous best result from the literature
      by 6 mm mean per-joint position error on Human3. 6M,
      corresponding to an error reduction of 11%, and the model
      also shows significant improvements on HumanEva-I.
      Moreover, experiments with back-projection show that it
      comfortably outperforms previous state-of-the-art results
      in semi-supervised settings where labeled data is scarce.
      Code and models are available.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2018
    authors: Sergey Edunov, Myle Ott, David Grangier, Michael Auli
    title: Understanding Back-Translation at Scale
    doc-url: https://arxiv.org/pdf/1808.09381
    code: https://github.com/pytorch/fairseq
    img: backtranslate
    booktitle: Proc. of EMNLP
    booktitle-url: https://arxiv.org/abs/1806.00187
    venue: conference
    abstract: >
      An effective method to improve neural machine translation
      with monolingual data is to augment the parallel training
      corpus with back-translations of target language sentences.
      This work broadens the understanding of back-translation
      and investigates a number of methods to generate synthetic
      source sentences. We find that in all but resource poor
      settings back-translations obtained via sampling or noised
      beam outputs are most effective. Our analysis shows that
      sampling or noisy synthetic data gives a much stronger
      training signal than data generated by beam or greedy search.
      We also compare how synthetic data compares to genuine bitext
      and study various domain effects. Finally, we scale to
      hundreds of millions of monolingual sentences and achieve
      a new state of the art of 35 BLEU on the
      WMT'14 English-German test set.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2018
    authors: Myle Ott, Sergey Edunov, David Grangier, Michael Auli
    title: Scaling Neural Machine Translation
    doc-url: https://arxiv.org/pdf/1806.00187
    code: https://github.com/pytorch/fairseq
    img: scaling
    booktitle: Proc. of WMT
    booktitle-url: https://arxiv.org/abs/1806.00187
    venue: conference
    abstract: >
      Sequence to sequence learning models still require several days
      to reach state of the art performance on large benchmark datasets
      using a single machine. This paper shows that reduced precision
      and large batch training can speedup training by nearly 5x on
      a single 8-GPU machine with careful tuning and implementation.
      On WMT'14 English-German translation, we match the accuracy
      of Vaswani et al. (2017) in under 5 hours when training
      on 8 GPUs and we obtain a new state of the art of 29.3 BLEU
      after training for 91 minutes on 128 GPUs.
      We further improve these results to 29.8 BLEU by training
      on the much larger Paracrawl dataset.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2018
    authors: Dario Pavllo, David Grangier, Michael Auli
    title: 'QuaterNet: A Quaternion-based Recurrent Model for Human Motion'
    doc-url: https://arxiv.org/pdf/1805.06485.pdf
    code: https://github.com/facebookresearch/QuaterNet
    img: quaternet
    booktitle: Proc. of BMVC
    booktitle-url: https://arxiv.org/abs/1805.06485
    venue: conference
    abstract: >
      Deep learning for predicting or generating 3D human pose sequences
      is an active research area. Previous work regresses either joint
      rotations or joint positions. The former strategy is prone to error
      accumulation along the kinematic chain, as well as discontinuities
      when using Euler angle or exponential map parameterizations.
      The latter requires re-projection onto skeleton constraints to avoid
      bone stretching and invalid configurations. This work addresses both
      limitations. Our recurrent network, QuaterNet, represents rotations
      with quaternions and our loss function performs forward kinematics
      on a skeleton to penalize absolute position errors instead of angle
      errors. On short-term predictions, QuaterNet improves the
      state-of-the-art quantitatively. For long-term generation, our
      approach is qualitatively judged as realistic as recent neural
      strategies from the graphics literature.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2018
    authors: Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato
    title: Analyzing Uncertainty in Neural Machine Translation
    doc-url: https://arxiv.org/pdf/1803.00047
    img: uncertainty
    booktitle: Proc. of ICML
    booktitle-url: https://arxiv.org/abs/1803.00047
    venue: conference
    abstract: >
      Machine translation is a popular test bed for research in neural
      sequence-to-sequence models but despite much recent research,
      there is still a lack of understanding of these models.
      Practitioners report performance degradation with large beams,
      the under-estimation of rare words and a lack of diversity
      in the final translations. Our study relates some of these issues
      to the inherent uncertainty of the task, due to the existence of
      multiple valid translations for a single source sentence, and
      to the extrinsic uncertainty caused by noisy training data.
      We propose tools and metrics to assess how uncertainty in the data
      is captured by the model distribution and how it affects search
      strategies that generate translations. Our results show that
      search works remarkably well but that the models tend to spread
      too much probability mass over the hypothesis space.
      Next, we propose tools to assess model calibration and show how
      to easily fix some shortcomings of current models. We release
      both code and multiple human reference translations for two
      popular benchmarks.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2018
    authors: Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato
    title: Classical Structured Prediction Losses for Sequence to Sequence Learning
    doc-url: https://arxiv.org/pdf/1711.04956
    img: classicseqlvl
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2018/
    venue: conference
    code: https://github.com/pytorch/fairseq/tree/classic_seqlevel
    abstract: >
      There has been much recent work on training neural attention models
      at the sequence-level using either reinforcement learning-style methods
      or by optimizing the beam. In this paper, we survey a range of
      classical objective functions that have been widely used to train
      linear models for structured prediction and apply them to neural
      sequence to sequence models. Our experiments show that these losses
      can perform surprisingly well by slightly outperforming beam search
      optimization in a like for like setup. We also report new state of
      the art results on both IWSLT 2014 German-English translation as well
      as Gigaword abstractive summarization.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2017
    authors: Angela Fan, David Grangier, Michael Auli
    title: Controllable Abstractive Summarization
    doc-url: https://arxiv.org/pdf/1711.05217
    img: controlabs
    booktitle: arXiv:1711.05217
    booktitle-url: https://arxiv.org/abs/1711.05217
    venue: conference
    abstract: >
      Current models for document summarization ignore user preferences
      such as the desired length, style or entities that the user has
      a preference for. We present a neural summarization model that
      enables users to specify such high level attributes in order
      to control the shape of the final summaries to better suit
      their needs. With user input, we show that our system can produce
      high quality summaries that are true to user preference.
      Without user input, we can set the control variables automatically
      and outperform comparable state of the art summarization systems
      despite the relative simplicity of our model.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2018
    authors: David Grangier, Michael Auli
    title: QuickEdit&#58; Editing Text &amp; Translations via Simple Delete Actions
    doc-url: https://arxiv.org/pdf/1711.04805
    img: quickedit
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2018/
    venue: conference
    abstract: >
      We propose a framework for computer-assisted text editing.
      It applies to translation post-editing and to paraphrasing and
      relies on very simple interactions. A human editor modifies a sentence
      by marking tokens they would like the system to change.
      Our model then generates a new sentence which reformulates
      the initial sentence by avoiding the words from the marked tokens.
      Our approach builds upon neural sequence-to-sequence modeling and
      introduces a neural network which takes as input a sentence along
      with deleted token markers. Our model is trained on translation
      bi-text by simulating post-edits. Our results on post-editing
      for machine translation and paraphrasing evaluate the performance
      of our approach. We show +11.4 BLEU with limited post-editing effort
      on the WMT-14 English-German translation task (25.2 to 36.6),
      which represents +5.9 BLEU over the post-editing baseline (30.7 to 36.6).
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2017
    authors: Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin
    title: Convolutional Sequence to Sequence Learning
    doc-url: https://arxiv.org/pdf/1705.03122
    img: fconv
    booktitle: Proc. of ICML
    booktitle-url:
    venue: conference
    code: https://github.com/facebookresearch/fairseq
    abstract: >
      The prevalent approach to sequence to sequence learning maps
      an input sequence to a variable length output sequence via
      recurrent neural networks. We introduce an architecture based
      entirely on convolutional neural networks. Compared to
      recurrent models, computations over all elements can be fully
      parallelized during training and optimization is easier since
      the number of non-linearities is fixed and independent of the
      input length. Our use of gated linear units eases gradient
      propagation and we equip each decoder layer with a separate
      attention module. We outperform the accuracy of the deep
      LSTM setup of Wu et al. (2016) on both WMT'14 English-German and
      WMT'14 English-French translation at an order of magnitude
      faster speed, both on GPU and CPU.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2017
    authors: Yann N. Dauphin, Angela Fan, Michael Auli and David Grangier
    title: Language Modeling with Gated Convolutional Networks
    doc-url: papers/gcnn.pdf
    img: gcnn
    booktitle: Proc. of ICML
    booktitle-url:
    venue: conference
    abstract: >
      The pre-dominant approach to language modeling to date
      is based on recurrent neural networks.
      In this paper we present a convolutional approach
      to language modeling. We introduce a novel
      gating mechanism that eases gradient propagation
      and which performs better than the LSTM-style
      gating of Oord et al. (2016b) despite being
      simpler. We achieve a new state of the art on
      WikiText-103 as well as a new best single-GPU
      result on the Google Billion Word benchmark.
      In settings where latency is important, our model
      achieves an order of magnitude speed-up compared
      to a recurrent baseline since computation
      can be parallelized over time. To our knowledge,
      this is the first time a non-recurrent approach outperforms
      strong recurrent models on these tasks.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2017
    authors: Jonas Gehring, Michael Auli, David Grangier, Yann N. Dauphin
    title: A Convolutional Encoder Model for Neural Machine Translation
    doc-url: papers/convenc.pdf
    img: convenc
    booktitle: Proc. of ACL
    booktitle-url:
    venue: conference
    abstract: >
      The prevalent approach to neural machine translation
      relies on bi-directional LSTMs to encode the source sentence.
      In this paper we present a faster and conceptually simpler
      architecture based on a succession of convolutional layers.
      This allows to encode the entire source sentence simultaneously
      compared to recurrent networks for which computation is
      constrained by temporal dependencies.
      We achieve a new state-of-the-art on WMT'16 English-Romanian
      translation and outperform several recently published
      results on the WMT'15 English-German task.
      We also achieve almost the same accuracy as a very deep LSTM setup
      on WMT'14 English-French translation.
      Our convolutional encoder speeds up CPU decoding by more than
      two times at the same or higher accuracy as a strong
      bi-directional LSTM baseline.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Roman Novak, Michael Auli, David Grangier
    title: Iterative Refinement for Machine Translation
    doc-url: papers/refine.pdf
    img: refine
    booktitle: arXiv:1610.06602
    booktitle-url:
    venue: conference
    abstract: >
      Existing machine translation decoding algorithms generate
      translations in a strictly monotonic fashion and never revisit
      previous decisions.
      As a result, earlier mistakes cannot be corrected at a later stage.
      In this paper, we present a translation scheme that starts
      from an initial guess and then makes iterative improvements
      that may revisit previous decisions.
      We parameterize our model as a convolutional neural network
      that predicts discrete substitutions to an existing translation
      based on an attention mechanism over both the source sentence
      as well as the current translation output.
      By making less than one modification per sentence,
      we improve the output of a phrase-based translation system
      by up to 0.4 BLEU on WMT15 German-English translation.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Gurvan L'Hostis, David Grangier, Michael Auli
    title: Vocabulary Selection Strategies for Neural Machine Translation
    doc-url: papers/vocabsel.pdf
    img: vocabsel
    booktitle: arXiv:1610.00072
    booktitle-url:
    venue: conference
    abstract: >
      Classical translation models constrain the space of possible outputs
      by selecting a subset of translation rules based on the input sentence.
      Recent work on improving the efficiency of neural translation models
      adopted a similar strategy by restricting the output vocabulary
      to a subset of likely candidates given the source.
      In this paper we experiment with context and embedding-based
      selection methods and extend previous work by examining speed and
      accuracy trade-offs in more detail.
      We show that decoding time on CPUs can be reduced by up to 90% and
      training time by 25% on the WMT15 English-German and
      WMT16 English-Romanian tasks at the same or only negligible
      change in accuracy.
      This brings the time to decode with a state of the art
      neural translation system to just over 140 msec per sentence
      on a single CPU core for English-German.

  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Remi Lebret, David Grangier, and Michael Auli
    title: Neural Text Generation from Structured Data with Application to the Biography Domain
    doc-url: papers/emnlp2016_biogen.pdf
    img: emnlp2016_biogen
    booktitle: Proc. of EMNLP
    booktitle-url: http://emnlp2016.net
    data: http://github.com/DavidGrangier/wikipedia-biography-dataset
    venue: conference
    abstract: >
      This paper introduces a neural model for concept-to-text
      generation that scales to large, rich domains.
      It generates biographical sentences from fact tables on
      a new dataset of biographies from Wikipedia.
      This set is an order of magnitude larger than existing
      resources with over 700k samples and a 400k vocabulary.
      Our model builds on conditional neural language models
      for text generation.
      To deal with the large vocabulary, we extend these models
      to mix a fixed vocabulary with copy actions that transfer
      sample-specific words from the input database to
      the generated output sentence.
      To deal with structured data, we allow the model to
      embed words differently depending on the data fields
      in which they occur.
      Our neural model significantly outperforms a Templated
      Kneser-Ney language model by nearly 15 BLEU.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Joel Legrand, Michael Auli, and Ronan Collobert
    title: Neural Network-based Word Alignment through Score Aggregation
    doc-url: papers/wmt2016_align.pdf
    img: wmt2016_align
    booktitle: Proc. of WMT
    booktitle-url: http://statmt.org/wmt16
    venue: conference
    abstract: >
      We present a simple neural network for word alignment
      that builds source and target word window representations
      to compute alignment scores for sentence pairs.
      To enable unsupervised training, we use an aggregation
      operation that summarizes the alignment scores for a
      given target word.
      A soft-margin objective increases scores for true target
      words while decreasing scores for target words that are
      not present.
      Compared to the popular Fast Align model, our approach
      improves alignment accuracy by 7 AER on English-Czech,
      by 6 AER on Romanian-English and by 1.7 AER on
      English-French alignment.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Wenlin Chen, David Grangier, Michael Auli
    title: Strategies for Training Large Vocabulary Neural Language Models
    doc-url: papers/acl2016_large_vocab_lm.pdf
    img: acl2016_large_vocab_lm
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2016.org
    venue: conference
    abstract: >
      Training neural network language models over large
      vocabularies is computationally costly compared to
      count-based models such as Kneser-Ney.
      We present a systematic comparison of neural strategies
      to represent and train large vocabularies, including
      softmax, hierarchical softmax, target sampling, noise
      contrastive estimation and self normalization.
      We extend self normalization to be a proper estimator
      of likelihood and introduce an efficient variant of
      softmax.
      We evaluate each method on three popular benchmarks,
      examining performance on rare words, the speed/accuracy
      trade-off and complementarity to Kneser-Ney.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Wenduan Xu, Michael Auli, and Stephen Clark
    title: Expected F-Measure Training for Shift-Reduce Parsing with Recurrent Neural Networks
    doc-url: papers/naacl2016_xf1_ccg.pdf
    img: naacl2016_xf1_ccg
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2016
    venue: conference
    abstract: >
      We present expected F-measure training for shift-reduce
      parsing with RNNs, which enables the learning of a global
      parsing model optimized for sentence-level F1.
      We apply the model to CCG parsing, where it improves
      over a strong greedy RNN baseline, by 1.47% F1, yielding
      state-of-the-art results for shift-reduce CCG parsing.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Sumit Chopra, Michael Auli, and Alexander M. Rush
    title: Abstractive Sentence Summarization with Attentive Recurrent Neural Networks
    doc-url: papers/naacl2016_summary_rnn.pdf
    img: naacl2016_summary_rnn
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2016
    venue: conference
    abstract: >
      Abstractive Sentence Summarization generates a shorter
      version of a given sentence while attempting to preserve
      its meaning.
      We introduce a conditional recurrent neural network (RNN)
      which generates a summary of an input sentence.
      The conditioning is provided by a novel convolutional
      attention-based encoder which ensures that the decoder
      focuses on the appropriate input words at each step of
      generation.
      Our model relies only on learned features and is easy to
      train in an end-to-end fashion on large data sets.
      Our experiments show that the model significantly outperforms
      the recently proposed state-of-the-art method on the
      Gigaword corpus while performing competitively on the
      DUC-2004 shared task.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba
    title: Sequence Level Training with Recurrent Neural Networks
    doc-url: papers/iclr2016_mixer.pdf
    img: iclr2016_mixer
    code: http://github.com/facebookresearch/MIXER
    booktitle: Proc. of ICLR
    booktitle-url: http://www.iclr.cc/doku.php?id=iclr2016:main
    venue: conference
    abstract: >
      Many natural language processing applications use language
      models to generate text.
      These models are typically trained to predict the next word
      in a sequence, given the previous words and some context
      such as an image.
      However, at test time the model is expected to generate the
      entire sequence from scratch.
      This discrepancy makes generation brittle, as errors may
      accumulate along the way.
      We address this issue by proposing a novel sequence level
      training algorithm that directly optimizes the metric used
      at test time, such as BLEU or ROUGE.
      On three different tasks, our approach outperforms several
      strong baselines for greedy generation.
      The method is also competitive when these baselines employ
      beam search, while being several times faster.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2015
    authors: Wenduan Xu, Michael Auli, and Stephen Clark
    title: CCG Supertagging with a Recurrent Neural Network
    doc-url: papers/super-rnn.pdf
    img: super-rnn
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2015.org
    venue: conference
    abstract: >
      Recent work on supertagging using a feed-forward neural network
      achieved significant improvements for CCG supertagging and
      parsing (Lewis and Steedman, 2014).
      However, their architecture is limited to considering local
      context and does not naturally model sequences of arbitrary
      length.
      In this paper, we show directly capturing sequence information
      using a recurrent neural network leads to further supertagging
      (up to 1.9%) and parsing accuracy improvements (up to 1% F1) on
      CCG-Bank, Wikipedia and biomedical text.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2015
    authors: Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao and Bill Dolan
    title: deltaBLEU&#58; A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets
    doc-url: papers/delta-bleu.pdf
    img: delta-bleu
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2015.org
    venue: conference
    abstract: >
      We introduce Discriminative BLEU (deltaBLEU), a novel metric
      for intrinsic evaluation of generated text in tasks that admit
      a diverse range of possible outputs.
      Reference strings are scored for quality by human raters on a
      scale of [-1.0, +1.0] to weight multi-reference BLEU.
      In tasks involving generation of conversational responses,
      deltaBLEU correlates reasonably with human judgments and outperforms
      sentence-level and IBM BLEU in terms of both Spearman's rho and
      Kendall's tau.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2015
    authors: Kai Zhao, Hany Hassan, and Michael Auli
    title: Learning Translation Models from Monolingual Continuous Representations
    doc-url: papers/dist_phrase_learn.pdf
    img: dist_phrase_learn
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2015
    venue: conference
    abstract: >
      The availability of parallel corpora limits the further success
      of statistical machine translation since infrequent phrases lack
      appropriate translation rules.
      Existing approaches that leverage the huge monolingual corpora
      usually need specially designed statistics for phrases, which
      take a long time to calculate.
      We propose a simple and fast translation model learning method
      that directly hallucinates translation rules for infrequent
      phrases from semantically similar neighbors, using
      continuous phrasal representations based on widely available
      word vectors.
      We also investigate approximated nearest neighbors query to
      further speed up search for semantically similar neighbors
      in continuous representation space.
      Experiments show that adding translation rules for infrequent phrases
      generated by our algorithm can improve accuracy by up to 1.5 BLEU.
      Compared to existing methods, our approach generates better
      translation rules, and is faster.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2015
    authors: Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Meg Mitchell, Jianfeng Gao, Bill Dolan, and Jian-Yun Nie
    title: A Neural Network Approach to Context-Sensitive Generation of Conversational Responses
    doc-url: papers/chitchat.pdf
    img: chitchat
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2015
    venue: conference
    abstract: >
      We present a novel response generation system that can be
      trained from end to end on large quantities of unstructured
      Twitter conversations.
      A neural network architecture is used to address sparsity issues
      that arise when integrating context information into classical
      statistical models, allowing the system to take into account
      previous dialog utterances.
      Our dynamic-context generative models show consistent gains over
      both context-sensitive and non-context-sensitive Machine Translation
      and Information Retrieval baselines.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2014
    authors: Michael Auli, Michel Galley, and Jianfeng Gao
    title: Large Scale Expected BLEU Training of Phrase-based Reordering Models
    doc-url: papers/expbleu-reorder.pdf
    img: expbleu-reorder
    booktitle: Proc. of EMNLP
    booktitle-url: http://emnlp2014.org/
    venue: conference
    abstract: >
      Recent work by Cherry (2013) has shown that directly
      optimizing phrase-based reordering models towards BLEU
      can lead to significant gains. Their approach is limited
      to small training sets of a few thousand sentences and
      a similar number of sparse features. We show how the
      expected BLEU objective allows us to train a simple
      linear discriminative reordering model with millions
      of sparse features on hundreds of thousands of sentences,
      each of which results in significant improvements.
      A comparison to likelihood training demonstrates that
      expected BLEU is vastly more effective. Our best results
      improve a hierarchical lexicalized reordering baseline
      by up to 2.0 BLEU in a single-reference setting on a
      French-English WMT 2012 setup.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2014
    authors: Michael Auli and Jianfeng Gao
    title: Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models
    doc-url: papers/expbleu-rnn.pdf
    img: expbleu-rnn
    booktitle: Proc. of ACL
    booktitle-url: http://acl2014.org
    venue: conference
    abstract: >
      Neural network language models are often trained by optimizing likelihood, but
      we would prefer to optimize for a task specific metric, such as BLEU for machine
      translation.
      We show how a recurrent neural network language model can be optimized towards
      an expected BLEU loss instead of the usual cross-entropy criterion.
      Furthermore, we tackle the issue of directly integrating a recurrent
      network into first-pass decoding under an efficient approximation.
      Our best results improve a phrase-based statistical machine translation
      system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the
      expected BLEU objective improves over a cross-entropy trained model by up to
      0.6 BLEU in a single reference setup.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2014
    authors: Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao
    title: Minimum Translation Modeling with Recurrent Neural Networks
    doc-url: papers/mtu-rnn.pdf
    img: mtu-rnn
    booktitle: Proc. of EACL
    booktitle-url: http://eacl2014.org
    venue: conference
    abstract: >
      We introduce recurrent neural network-based Minimum Translation Unit (MTU)
      models which make predictions based on an unbounded history of previous
      bilingual contexts.
      Traditional back-off n-gram models suffer under the sparse nature of MTUs which
      makes estimation of high-order sequence models challenging.
      We tackle the sparsity problem by modeling MTUs both as bags-of-words and as
      a sequence of individual source and target words.
      Our best results improve the output of a phrase-based statistical machine
      translation system trained on WMT 2012 French-English data by up to 1.5 BLEU,
      and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2013
    authors: Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig
    title: Joint Language and Translation Modeling with Recurrent Neural Networks
    doc-url: papers/rnn-joint-lm-tm.pdf
    img: rnn-joint-lm-tm
    booktitle: Proc. of EMNLP
    booktitle-url: http://hum.csse.unimelb.edu.au/emnlp2013
    venue: conference
    abstract: >
      We present a joint language and translation model based on a
      recurrent neural network which predicts target words based on
      an unbounded history of both source and target words. The weaker
      independence asssumptions of this model result in a vastly larger
      search space compared to related feed forward-based language or
      translation models. We tackle this issue with a new lattice rescoring
      algorithm and demonstrate its effectiveness empirically. Our joint
      model builds on a well known recurrent neural network language model
      (Mikolov, 2012) augmented by a layer of additional inputs from the
      source language. We show competitive accuracy compared to the
      traditional channel model features. Our best results improve the output
      of a system trained on WMT 2012 French-English data by up to 1.5 BLEU,
      and by 1.1 BLEU on average across several test sets.
  -
    layout: paper
    paper-type: phd-thesis
    selected: y
    year: 2012
    title: Integrated Supertagging and Parsing
    institution: University of Edinburgh
    doc-url: papers/michael.auli.thesis.pdf
    img: thesis2012
    abstract: >
      <p>Parsing is the task of assigning syntactic or semantic structure to a natural language sentence. This thesis focuses on syntactic parsing with Combinatory Categorial Grammar (CCG; Steedman 2000). CCG allows incremental processing, which is essential for speech recognition and some machine translation models, and it can build semantic structure in tandem with syntactic parsing. Supertagging solves a subset of the parsing task by assigning lexical types to words in a sentence using a sequence model. It has emerged as a way to improve the efficiency of full CCG parsing (Clark and Curran, 2007) by reducing the parser's search space. This has been very successful and it is the central theme of this thesis.

      <p>We begin by an analysis of how efficiency is being traded for accuracy in supertagging. Pruning the search space by supertagging is inherently approximate and to contrast this we include A* in our analysis, a classic exact search technique.Interestingly, we find that combining the two methods improves efficiency but we also demonstrate that excessive pruning by a supertagger significantly lowers the upper bound on accuracy of a CCG parser.

      <p>Inspired by this analysis, we design a single integrated model with both supertagging and parsing features, rather than separating them into distinctmodels chained together in a pipeline. To overcome the resulting complexity, we experiment with both loopy belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem.

      <p>Finally, we address training the integrated model. We adopt the idea of optimising directly for a task-specific metric such as is common in other areas like statistical machine translation. We demonstrate how a novel dynamic programming algorithm enables us to optimise for F-measure, our task-specific evaluation metric, and experiment with approximations, which prove to be excellent substitutions.

      <p>Each of the presented methods improves over the state-of-the-art in CCG parsing. Moreover, the improvements are additive, achieving a labelled/unlabelled dependency F-measure on CCGbank of 89.3%/94.0% with gold part-of-speech tags, and 87.2%/92.8% with automatic part-of-speech tags, the best reported results for this task to date. Our techniques are general and we expect them to apply to other parsing problems, including lexicalised tree adjoining grammar and context-free grammar parsing.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: Training a Log-Linear Parser with Loss Functions via Softmax-Margin
    doc-url: papers/softmax-ccg.pdf
    img: softmax-ccg
    booktitle: Proc. of EMNLP
    booktitle-url: http://conferences.inf.ed.ac.uk/emnlp2011/
    venue: conference
    abstract: >
      Log-linear parsing models are often trained by optimizing
      likelihood, but we would prefer to optimize for a task-specific metric like F-measure.
      Softmax-margin is a convex objective for such models that minimizes a bound on
      expected risk for a given loss function, but its nave application requires the loss
      to decompose over the predicted structure, which is not true of F-measure.
      We use softmax-margin to optimize a log-linear CCG parser for a variety of loss functions, and
      demonstrate a novel dynamic programming algorithm that enables us to use it with
      F-measure, leading to substantial gains in accuracy on CCGBank.  When we embed our
      loss-trained parser into a larger model that includes supertagging features
      incorporated via belief propagation, we obtain further improvements and achieve
      a labelled/unlabelled dependency
      F-measure of 89.3%/94.0% on gold part-of-speech tags,
      and 87.2%/92.8% on automatic part-of-speech
      tags, the best reported results for this task.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing
    doc-url: papers/lbp-vs-dd.pdf
    img: lbp-vs-dd
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2011.org/
    venue: conference
    abstract: >
      Via an oracle experiment, we show that the upper bound on accuracy of a CCG
      parser is significantly lowered when its search space is pruned using a
      supertagger, though the supertagger also prunes many bad parses.  Inspired by
      this analysis, we design a single model with both supertagging and parsing
      features, rather than separating them into distinct models chained together
      in a pipeline.  To overcome the resulting increase in complexity, we
      experiment with both belief propagation and dual decomposition approaches to
      inference, the first empirical comparison of these algorithms that we are
      aware of on a structured natural language processing problem.  On CCGbank we
      achieve a labelled dependency F-measure of 88.8% on gold POS tags, and
      86.7% on automatic part-of-speech tags, the best reported results for this
      task.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: Efficient CCG Parsing&#58; A* versus Adaptive Supertagging
    doc-url: papers/astar-ccg.pdf
    img: astar-ccg
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2011.org/
    venue: conference
    abstract: >
      We present a systematic comparison and combination of two orthogonal techniques for efficient
      parsing of Combinatory Categorial Grammar (CCG).  First we consider adaptive supertagging, a
      widely used approximate search technique that prunes most lexical categories from the parser's
      search space using a separate sequence model.  Next we consider several variants on A*, a
      classic exact search technique which to our knowledge has not been applied to more expressive
      grammar formalisms like CCG.  In addition to standard hardware-independent measures of parser
      effort we also present what we believe is the first evaluation of A* parsing on the more
      realistic but more stringent metric of CPU time.  By itself, A* substantially reduces parser
      effort as measured by the number of edges considered during parsing, but we show that for CCG
      this does not always correspond to improvements in CPU time over a CKY baseline.  Combining A*
      with adaptive supertagging decreases CPU time by 15% for our best model.
  -
    layout: paper
    paper-type: firstyearreport
    selected: y
    year: 2009
    institution: University of Edinburgh
    title: CCG-based Models for Statistical Machine Translation
    doc-url: papers/first-year-report.pdf
    img: first-year-report
    abstract: >
      <p>The arguably best performing statistical machine translation systems are based
      on context-free formalisms or weakly equivalent ones. These models usually use
      a synchronous version of a context-free grammar (SCFG) which we argue is too
      rigid for the highly ambiguous task of human language translation. This is
      exacerbated by the fact that the imperfect methods available for aligning
      parallel texts make extracting an efficient grammar very hard. As a result,
      the context-free grammars extracted are usually very large in size after
      having already been restricted through a variety of constraints.

      <p>We propose to use Combinatorial Categorial Grammar (CCG) for machine translation
      models. CCG is a lexicalized, mildly-context-sensitive formalism which
      is very well suited to capture long-distance dependencies that are not
      addressed very well by most current models. We believe that CCG is very well
      suited for the task of machine translation due to its ability to represent
      non-constituents in a syntactic way which frequently occur in parallel texts
      as well as its high derivational flexibility. This allows us to use some of
      the advantages of non-syntactic phrase-based approaches within a syntactic
      framework such as a relatively small grammar size compared to
      context-free-based machine translation grammars.

      <p>A number of models leveraging the advantages of CCG are possible, however, our
      principal goal is to develop a string-to-tree based model which projects CCG
      on the target side of a synchronous grammar. We intend to apply the vast
      progress made in monolingual CCG parsing to machine translation. Additionally,
      we propose to extend CCG to a synchronous grammar (SCCG) as it has been done
      for other related formalisms such as tree adjoining grammars. We hope that a
      SCCG may provide similar derivational flexibility to monolingual CCG which may
      result in a better model for translational equivalence.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2009
    authors: Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn
    title: A Systematic Analysis of Translation Model Search Spaces
    doc-url: http://www.aclweb.org/anthology/W/W09/W09-0437.pdf
    booktitle: Proc. of WMT
    booktitle-url: http://www.statmt.org/wmt09/
    img: wmt2009
    venue: workshop
    abstract: >
      Translation systems are complex, and most metrics do little to pinpoint causes of error or isolate system differences.  We use a simple technique to discover induction errors, which occur when good translations are absent from model search spaces.  Our results show that a common pruning heuristic drastically increases induction error, and also strongly suggest that the search spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences.
