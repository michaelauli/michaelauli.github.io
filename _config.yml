exclude: ['README.md']
timezone: US/Pacific
talks:
  -
    title: Unified self-supervised learning for speech, vision and NLP
    detail: Talk at ECCV Workshop on Perception
    url: "talks/data2vec.pdf"
    img: "data2vec.png"   
  -
    title: wav2vec&#58; Self-supervised learning of speech representations
    detail: Talk at MIT, CMU, U of Edinburgh, Spring 2021.
    url: "talks/wav2vec-ssl.pdf"
    img: "wav2vec-ssl.png"
  -
    title: Efficient Sequence Modeling
    detail: Talk at WNGT'19, Stanford, Berkeley, Nov 2019.
    url: "talks/efficient-seq-modeling.pdf"
    img: "efficient-seq-modeling.png"
  -
    title: Sequence to Sequence Learning&#58; Fast Training and Inference with Gated Convolutions
    detail: Talk at Johns Hopkins University, Oct 2017.
    url: "talks/convs2s.pdf"
    img: "convs2s.png"
  -
    title: Learning to translate with neural networks
    detail: Talk at Facebook, Google, Amazon and the University of Washington, 2014.
    url: "talks/facebook-rnn-translate.pdf"
    img: "facebook-rnn-translate.jpg"
  -
    title: Integrated Parsing and Tagging
    detail: Talk at Carnegie Mellon University, Johns Hopkins University, BBN Technologies, IBM Research and Microsoft Research, 2011.
    url: "talks/parsing-tagging-talk.pdf"
    img: "parsing-tagging-talk.jpg"
press:
  -
    title: Meta researchers build an AI that learns equally well from visual, written or spoken materials
    detail: TechCrunch, 20 Jan 2022.
    url: https://techcrunch.com/2022/01/20/meta-researchers-build-an-ai-that-learns-equally-well-from-visual-written-or-spoken-materials
  -
    title: Metaâ€™s new learning algorithm can teach AI to multi-task
    detail: MIT Technology Review, 20 Jan 2022.
    url: https://www.technologyreview.com/2022/01/20/1043885/meta-ai-facebook-learning-algorithm-nlp-vision-speech-agi/
  -
    title: Facebook AI cuts by more than half the error rate of unsupervised speech recognition
    detail: ZDNet, 21 May 2021.
    url: https://www.zdnet.com/article/facebook-ai-cuts-by-more-than-half-error-rate-of-unsupervised-speech-recognition/
  -
    title: Facebook Wav2vec-U learns to recognize speech from unlabeled data
    detail: Venturebeat, 21 May 2021.
    url: https://venturebeat.com/2021/05/21/facebook-wav2vec-u-learns-to-recognize-speech-from-unlabeled-data/
  -
    title: Facebook claims wav2vec 2.0 tops speech recognition performance with 10 minutes of labeled data
    detail: Venturebeat, 23 June 2020.
    url: https://venturebeat.com/2020/06/23/facebook-claims-wav2vec-2-0-tops-speech-recognition-performance-with-10-minutes-of-labeled-data/
  -
    title: Facebook details wav2vec, an AI algorithm that uses raw audio to improve speech recognition.
    detail: Venturebeat, 5 Nov 2019.
    url: https://venturebeat.com/2019/11/05/facebook-details-wav2vec-an-ai-algorithm-that-uses-raw-audio-to-improve-speech-recognition/
  -
    title: Facebook's new AI could lead to translations that actually make sense.
    detail: Wired, 9 May 2017.
    url: https://www.wired.com/2017/05/facebook-open-sources-neural-networks-speed-translations/
papers:
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2022
    title: "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"
    authors: Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.
    doc-url: https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language
    img: data2vec
    booktitle: arXiv
    booktitle-url: https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language
    code: https://github.com/pytorch/fairseq/tree/master/examples/data2vec/
    blog: https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text
    venue: conference
    abstract: >
      While the general idea of self-supervised learning is identical
      across modalities, the actual algorithms and objectives differ
      widely because they were developed with a single modality in mind.
      To get us closer to general self-supervised learning, we present
      data2vec, a framework that uses the same learning method for
      either speech,NLP or computer vision. The core idea is to
      predict latent representations of the full input data based on
      a masked view of the input in a selfdistillation setup using a
      standard Transformer architecture. Instead of predicting
      modality-specific targets such as words, visual tokens or units
      of human speech which are local in nature, data2vec predicts
      contextualized latent representations that contain information
      from the entire input. Experiments on the major benchmarks of
      speech recognition, image classification, and natural language
      understanding demonstrate a new state of the art or competitive
      performance to predominant approaches.
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2021
    title: "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"
    authors: Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli
    doc-url: https://arxiv.org/pdf/2111.09296.pdf
    img: xlsr2
    booktitle: arXiv
    booktitle-url: https://arxiv.org/abs/2111.09296
    code: https://github.com/pytorch/fairseq/tree/master/examples/wav2vec/
    blog: https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages
    venue: conference
    abstract: >
      This paper presents XLS-R, a large-scale model for cross-lingual
      speech representation learning based on wav2vec 2.0. We train
      models with up to 2B parameters on nearly half a million hours
      of publicly available speech audio in 128 languages, an order
      of magnitude more public data than the largest known prior work.
      Our evaluation covers a wide range of tasks, domains, data
      regimes and languages, both high and low-resource. On the
      CoVoST-2 speech translation benchmark, we improve the previous
      state of the art by an average of 7.4 BLEU over 21 translation
      directions into English. For speech recognition, XLS-R improves
      over the best known prior work on BABEL, MLS, CommonVoice as
      well as VoxPopuli, lowering error rates by 14-34% relative on
      average. XLS-R also sets a new state of the art on VoxLingua107
      language identification. Moreover, we show that with sufficient
      model size, cross-lingual pretraining can outperform
      English-only pretraining when translating English speech into
      other languages, a setting which favors monolingual pretraining.
      We hope XLS-R can help to improve speech processing tasks for
      many more languages of the world.
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2021
    title: "Unsupervised Speech Recognition"
    authors: Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, Michael Auli
    doc-url: https://arxiv.org/pdf/2105.11084
    img: w2vu
    booktitle: Proc. of NeurIPS
    booktitle-url: https://arxiv.org/abs/2105.11084
    code: https://github.com/pytorch/fairseq/tree/master/examples/wav2vec/unsupervised
    blog: https://ai.facebook.com/blog/wav2vec-unsupervised-speech-recognition-without-supervision/
    venue: conference
    abstract: >
      Despite rapid progress in the recent past, current speech
      recognition systems still require labeled training data which
      limits this technology to a small fraction of the languages
      spoken around the globe. This paper describes wav2vec-U, short
      for wav2vec Unsupervised, a method to train speech recognition
      models without any labeled data. We leverage self-supervised
      speech representations to segment unlabeled audio and learn a
      mapping from these representations to phonemes via adversarial
      training. The right representations are key to the success of our
      method. Compared to the best previous unsupervised work,
      wav2vec-U reduces the phoneme error rate on the TIMIT benchmark
      from 26.1 to 11.3. On the larger English Librispeech benchmark,
      wav2vec-U achieves a word error rate of 5.9 on test-other,
      rivaling some of the best published systems trained on 960 hours
      of labeled data from only two years ago. We also experiment on
      nine other languages, including low-resource languages such as
      Kyrgyz, Swahili and Tatar.
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2020
    title: "Beyond english-centric multilingual machine translation"
    authors: Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli*, Armand Joulin*
    doc-url: https://arxiv.org/pdf/2010.11125
    img: m2m100
    booktitle: JMLR
    booktitle-url: https://arxiv.org/abs/2010.11125
    code: https://github.com/pytorch/fairseq/tree/master/examples/m2m_100
    blog: https://ai.facebook.com/blog/introducing-many-to-many-multilingual-machine-translation/
    venue: journal
    abstract: >
      Existing work in translation demonstrated the potential of
      massively multilingual machine translation by training a single
      model able to translate between any pair of languages. However,
      much of this work is English-Centric by training only on data
      which was translated from or to English. While this is supported
      by large sources of training data, it does not reflect
      translation needs worldwide. In this work, we create a true
      Many-to-Many multilingual translation model that can translate
      directly between any pair of 100 languages. We build and open
      source a training dataset that covers thousands of language
      directions with supervised data, created through large-scale
      mining. Then, we explore how to effectively increase model
      capacity through a combination of dense scaling and
      language-specific sparse parameters to create high quality
      models. Our focus on non-English-Centric models brings gains of
      more than 10 BLEU when directly translating between non-English
      directions while performing competitively to the best single
      systems of WMT. We open-source our scripts so that others may
      reproduce the data, evaluation, and final M2M-100 model.
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2020
    title: "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"
    authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli
    doc-url: https://arxiv.org/pdf/2006.11477
    img: wav2vec2
    booktitle: NeurIPS
    booktitle-url: https://arxiv.org/abs/2006.11477
    code: https://github.com/pytorch/fairseq/tree/master/examples/wav2vec
    blog: https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/
    venue: conference
    abstract: >
      We show for the first time that learning powerful representations from
      speech audio alone followed by fine-tuning on transcribed speech can
      outperform the best semi-supervised methods while being conceptually
      simpler. wav2vec 2.0 masks the speech input in the latent space and
      solves a contrastive task defined over a quantization of the latent
      representations which are jointly learned. We set a new state of the art
      on both the 100 hour subset of Librispeech as well as on TIMIT phoneme
      recognition. When lowering the amount of labeled data to one hour, our
      model outperforms the previous state of the art on the 100 hour subset
      while using 100 times less labeled data. Using just ten minutes of
      labeled data and pre-training on 53k hours of unlabeled data still
      achieves 5.7/10.1 WER on the noisy/clean test sets of Librispeech.
      This demonstrates the feasibility of speech recognition with limited
      amounts of labeled data. Fine-tuning on all of Librispeech achieves
      1.9/3.5 WER using a simple baseline model architecture. We will release
      code and models.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2021
    title: "Unsupervised Cross-lingual Representation Learning for Speech Recognition"
    authors: Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli
    doc-url: https://arxiv.org/pdf/2006.13979
    img: xlsr
    booktitle: Interspeech
    booktitle-url: https://arxiv.org/abs/2006.13979
    code:
    venue: conference
    abstract: >
      This paper presents XLSR which learns cross-lingual speech representations
      by pretraining a single model from the raw waveform of speech in multiple
      languages. We build on a concurrently introduced self-supervised model
      which is trained by solving a contrastive task over masked latent speech
      representations and jointly learns a quantization of the latents shared
      across languages. The resulting model is fine-tuned on labeled data and
      experiments show that cross-lingual pretraining significantly outperforms
      monolingual pretraining. On the CommonVoice benchmark, XLSR shows a
      relative phoneme error rate reduction of 72% compared to the best known
      results. On BABEL, our approach improves word error rate by 16% relative
      compared to the strongest comparable system. Our approach enables a
      single multilingual speech recognition model which is competitive to
      strong individual models. Analysis shows that the latent discrete speech
      representations are shared across languages with increased sharing for
      related languages.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2020
    title: "Robust and On-the-fly Dataset Denoising for Image Classification"
    authors: Jiaming Song, Lunjia Hu, Yann Dauphin, Michael Auli, Tengyu Ma
    doc-url: https://arxiv.org/pdf/2003.10647.pdf
    img: odd
    booktitle: Proc. of ECCV
    booktitle-url: https://arxiv.org/abs/2003.10647
    code:
    venue: conference
    abstract: >
      Memorization in over-parameterized neural networks could severely hurt
      generalization in the presence of mislabeled examples. However, mislabeled
      examples are hard to avoid in extremely large datasets collected with weak
      supervision. We address this problem by reasoning counterfactually about
      the loss distribution of examples with uniform random labels had they were
      trained with the real examples, and use this information to remove noisy
      examples from the training set. First, we observe that examples with
      uniform random labels have higher losses when trained with stochastic
      gradient descent under large learning rates. Then, we propose to model
      the loss distribution of the counterfactual examples using only the
      network parameters, which is able to model such examples with remarkable
      success. Finally, we propose to remove examples whose loss exceeds a
      certain quantile of the modeled loss distribution. This leads to
      On-the-fly Data Denoising (ODD), a simple yet effective algorithm that
      is robust to mislabeled examples, while introducing almost zero
      computational overhead compared to standard training. ODD is able to
      achieve state-ofthe-art results on a wide range of datasets including
      real-world ones such as WebVision and Clothing1M.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2020
    title: "Effectiveness of self-supervised pre-training for speech recognition"
    authors: Alexei Baevski, Michael Auli, Abdelrahman Mohamed
    doc-url: https://arxiv.org/pdf/1911.03912
    img: effectiveness
    booktitle: Proc. of ICASSP
    booktitle-url: https://arxiv.org/abs/1911.03912
    code:
    venue: conference
    abstract: >
      We compare self-supervised representation learning algorithms which
      either explicitly quantize the audio data or learn representations
      without quantization. We find the former to be more accurate since it
      builds a good vocabulary of the data through vq-wav2vec to enable
      learning of effective representations in subsequent BERT training.
      Different to previous work, we directly fine-tune the pre-trained BERT
      models on transcribed speech using a Connectionist Temporal
      Classification (CTC) loss instead of feeding the representations into
      a task-specific model. We also propose a BERT-style model learning
      directly from the continuous audio data and compare pre-training on
      raw audio to spectral features. Fine-tuning a BERT model on 10 hours
      of labeled Librispeech data with a vq-wav2vec vocabulary is almost
      as good as the best known reported system trained on 100 hours of
      labeled data on test-clean, while achieving a 25% WER reduction on
      test-other. When using only 10 minutes of labeled data, WER is 25.2
      on test-other and 16.3 on test-clean. This demonstrates that
      self-supervision can enable speech recognition systems trained
      on a near-zero amount of transcribed data.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2020
    title: "Depth-Adaptive Transformer"
    authors: Maha Elbayad, Jiatao Gu, Edouard Grave, Michael Auli
    doc-url: https://arxiv.org/pdf/1910.10073
    img: depthadaptive
    booktitle: Proc. of ICLR
    booktitle-url: https://arxiv.org/abs/1910.10073
    code:
    venue: conference
    abstract: >
      State of the art sequence-to-sequence models perform a fixed number of
      computations for each input sequence regardless of whether it is easy or
      hard to process. In this paper, we train Transformer models which can make
      output predictions at different stages of the network and we investigate
      different ways to predict how much computation is required for a
      particular sequence. Unlike dynamic computation in Universal Transformers,
      which applies the same set of layers iteratively, we apply different
      layers at every step to adjust both the amount of computation as well as
      the model capacity. Experiments on machine translation benchmarks show
      that this approach can match the accuracy of a baseline Transformer while
      using only half the number of decoder layers.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2020
    title: "On The Evaluation of Machine Translation Systems Trained With Back-Translation"
    authors: Sergey Edunov, Myle Ott, Marc'Aurelio Ranzato, Michael Auli
    doc-url: https://arxiv.org/pdf/1908.05204
    img: bteval
    booktitle: Proc. of ACL
    booktitle-url: https://arxiv.org/abs/1908.05204
    code:
    venue: conference
    abstract: >
      Back-translation is a widely used data augmentation technique which
      leverages target monolingual data. However, its effectiveness has been
      challenged since automatic metrics such as BLEU only show significant
      improvements for test examples where the source itself is a translation,
      or translationese. This is believed to be due to translationese inputs
      better matching the back-translated training data. In this work, we show
      that this conjecture is not empirically supported and that
      back-translation improves translation quality of both naturally occurring
      text as well as translationese according to professional human
      translators. We provide empirical evidence to support the view that
      back-translation is preferred by humans because it produces more fluent
      outputs. BLEU cannot capture human preferences because references are
      translationese when source sentences are natural text. We recommend
      complementing BLEU with a language model score to measure fluency.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2020
    title: "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations"
    authors: Alexei Baevski, Steffen Schneider, Michael Auli
    doc-url: https://arxiv.org/pdf/1910.05453
    img: vqw2v
    booktitle: Proc. of ICLR
    booktitle-url: https://arxiv.org/abs/1910.05453
    code:
    venue: conference
    abstract: >
      We propose vq-wav2vec to learn discrete representations of audio segments
      through a wav2vec-style self-supervised context prediction task. The
      algorithm uses either a gumbel softmax or online k-means clustering to
      quantize the dense representations. Discretization enables the direct
      application of algorithms from the NLP community which require discrete
      inputs. Experiments show that BERT pre-training achieves a new state of
      the art on TIMIT phoneme classification and WSJ speech recognition.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    title: "The Source-Target Domain Mismatch Problem in Machine Translation"
    authors: Jiajun Shen, Peng-Jen Chen, Matt Le, Junxian He, Jiatao Gu, Myle Ott, Michael Auli, Marc'Aurelio Ranzato
    doc-url: https://arxiv.org/pdf/1909.13151
    img: srctgtmismatch
    booktitle: arXiv
    booktitle-url: https://arxiv.org/abs/1909.13151
    code:
    venue: conference
    abstract: >
      While we live in an increasingly interconnected world, different places
      still exhibit strikingly different cultures and many events we experience
      in our every day life pertain only to the specific place we live in. As a
      result, people often talk about different things in different parts of the
      world. In this work we study the effect of local context in machine
      translation and postulate that particularly in low resource settings this
      causes the domains of the source and target language to greatly mismatch,
      as the two languages are often spoken in further apart regions of the
      world with more distinctive cultural traits and unrelated local events. In
      this work we first propose a controlled setting to carefully analyze the
      source-target domain mismatch, and its dependence on the amount of
      parallel and monolingual data. Second, we test both a model trained with
      back-translation and one trained with self-training. The latter leverages
      in-domain source monolingual data but uses potentially incorrect target
      references. We found that these two approaches are often complementary to
      each other. For instance, on a low-resource Nepali-English dataset the
      combined approach improves upon the baseline using just parallel data by
      2.5 BLEU points, and by 0.6 BLEU point when compared to back-translation.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    title: "Simple and effective noisy channel modeling for neural machine translation"
    authors: Kyra Yee, Nathan Ng, Yann N Dauphin, Michael Auli
    doc-url: https://arxiv.org/pdf/1908.05731
    img: noisychannel
    booktitle: Proc. of EMNLP
    booktitle-url: https://arxiv.org/abs/1908.05731
    code: https://github.com/pytorch/fairseq/tree/master/examples/noisychannel
    venue: conference
    abstract: >
      Previous work on neural noisy channel modeling relied on latent variable
      models that incrementally process the source and target sentence. This
      makes decoding decisions based on partial source prefixes even though the
      full source is available. We pursue an alternative approach based on
      standard sequence to sequence models which utilize the entire source.
      These models perform remarkably well as channel models, even though they
      have neither been trained on, nor designed to factor over incomplete
      target sentences. Experiments with neural language models trained on
      billions of words show that noisy channel models can outperform a direct
      model by up to 3.2 BLEU on WMT'17 German-English translation. We evaluate
      on four language-pairs and our channel models consistently outperform
      strong alternatives such right-to-left reranking models and ensembles of
      direct models.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    title: "ELI5: Long Form Question Answering"
    authors: Angela Fan, Yacine Jernite, Ethan Perez, Jason Weston, Michael Auli
    doc-url: https://arxiv.org/pdf/1907.09190
    img: eli5
    booktitle: Proc. of ACL
    booktitle-url: https://arxiv.org/abs/1907.09190
    code:
    data: https://github.com/facebookresearch/ELI5
    blog: https://ai.facebook.com/blog/longform-qa/
    venue: conference
    abstract: >
      We introduce the first large-scale corpus for long-form question
      answering, a task requiring elaborate and in-depth answers to
      open-ended questions. The dataset comprises 270K threads from
      the Reddit forum ``Explain Like I'm Five'' (ELI5) where an
      online community provides answers to questions which are
      comprehensible by five year olds.  Compared to existing
      datasets, ELI5 comprises diverse questions requiring
      multi-sentence answers. We provide a large set of web documents
      to help answer the question. % in order to find support for the
      answer. Automatic and human evaluations show that an abstractive
      model trained with a multi-task objective outperforms
      conventional Seq2Seq, language modeling, as well as a strong
      extractive baseline.  However, our best model is still far from
      human performance since raters prefer gold responses in over
      86\% of cases, leaving ample opportunity for future improvement.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    title: "Facebook FAIR's WMT19 News Translation Task Submission"
    authors: Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov
    doc-url: https://arxiv.org/pdf/1907.06616
    img: wmt19
    booktitle: Proc. of WMT
    booktitle-url: https://arxiv.org/abs/1907.06616
    code: https://github.com/pytorch/fairseq/tree/master/examples/wmt19
    blog: https://ai.facebook.com/blog/facebook-leads-wmt-translation-competition/
    venue: conference
    abstract: >
      This paper describes Facebook FAIR's submission to the WMT19 shared news
      translation task. We participate in two language pairs and four language
      directions, English<-> German and English<-> Russian. Following our
      submission from last year, our baseline systems are large BPE-based
      transformer models trained with the Fairseq sequence modeling toolkit
      which rely on sampled back-translations. This year we experiment with
      different bitext data filtering schemes, as well as with adding filtered
      back-translated data. We also ensemble and fine-tune our models on
      domain-specific data, then decode using noisy channel model reranking. Our
      submissions are ranked first in all four directions of the human
      evaluation campaign. On En-> De, our system significantly outperforms
      other systems as well as human translations. This system improves upon our
      WMT'18 submission by 4.5 BLEU points.
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2019
    title: "wav2vec: Unsupervised Pre-training for Speech Recognition"
    authors: Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli
    doc-url: https://arxiv.org/pdf/1904.05862
    img: wav2vec
    booktitle: Proc. of Interspeech
    booktitle-url: https://arxiv.org/abs/1904.05862
    code: https://github.com/pytorch/fairseq/tree/master/examples/wav2vec
    blog: https://ai.facebook.com/blog/wav2vec-state-of-the-art-speech-recognition-through-self-supervision/
    venue: conference
    abstract: >
      We explore unsupervised pre-training for speech recognition by
      learning representations of raw audio. wav2vec is trained on
      large amounts of unlabeled audio data and the resulting
      representations are then used to improve acoustic model
      training. We pre-train a simple multi-layer convolutional neural
      network optimized via a noise contrastive binary classification
      task. Our experiments on WSJ reduce WER of a strong
      character-based log-mel filterbank baseline by up to 32% when
      only a few hours of transcribed data is available. Our approach
      achieves 2.43% WER on the nov92 test set. This outperforms Deep
      Speech 2, the best reported character-based system in the
      literature while using three orders of magnitude less labeled
      training data.
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2019
    title: "fairseq: A fast, extensible toolkit for sequence modeling"
    authors: Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli
    doc-url: https://arxiv.org/pdf/1904.01038
    img: fairseq
    booktitle: Proc. of NAACL, Demonstrations
    booktitle-url: https://arxiv.org/abs/1904.01038
    code: https://github.com/pytorch/fairseq
    venue: conference
    abstract: >
      fairseq is an open-source sequence modeling toolkit that
      allows researchers and developers to train custom models
      for translation, summarization, language modeling, and
      other text generation tasks. The toolkit is based on PyTorch
      and supports distributed training across multiple GPUs
      and machines. We also support fast mixed-precision training
      and inference on modern GPUs. A demo video can be found
      here: https://www.youtube.com/watch?v=OtgDdWtHvto
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    title: "GLOSS: Generative Latent Optimization of Sentence Representations"
    authors: Sidak Pal Singh, Angela Fan, Michael Auli
    doc-url: https://arxiv.org/pdf/1907.06385
    img: gloss
    booktitle: Proc. of WMT
    booktitle-url: https://arxiv.org/abs/1907.06385
    code:
    blog:
    venue: conference
    abstract: >
      We propose a method to learn unsupervised sentence representations in a
      non-compositional manner based on Generative Latent Optimization. Our
      approach does not impose any assumptions on how words are to be combined
      into a sentence representation. We discuss a simple Bag of Words model as
      well as a variant that models word positions. Both are trained to
      reconstruct the sentence based on a latent code and our model can be used to
      generate text. Experiments show large improvements over the related
      Paragraph Vectors. Compared to uSIF, we achieve a relative improvement of 5%
      when trained on the same data and our method performs competitively to
      Sent2vec while trained on 30 times less data.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    title: "Pre-trained Language Model Representations for Language Generation"
    authors: Sergey Edunov, Alexei Baevski, Michael Auli
    doc-url: https://arxiv.org/pdf/1903.09722
    img: pretrain_langgen
    booktitle: Proc. of NAACL
    booktitle-url: https://arxiv.org/abs/1903.09722
    code: https://github.com/pytorch/fairseq
    venue: conference
    abstract: >
      Pre-trained language model representations have been
      successful in a wide range of language understanding tasks.
      In this paper, we examine different strategies to integrate
      pretrained representations into sequence to sequence models
      and apply it to neural machine translation and abstractive
      summarization. We find that pre-trained representations are
      most effective when added to the encoder network which slows
      inference by only 14%. Our experiments in machine translation
      show gains of up to 5.3 BLEU in a simulated resource-poor setup.
      While returns diminish with more labeled data, we still
      observe improvements when millions of sentence-pairs are
      available. Finally, on abstractive summarization we achieve
      a new state of the art on the full text version of
      CNN-DailyMail.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    title: "Cloze-driven Pretraining of Self-attention Networks"
    authors: Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli
    doc-url: https://arxiv.org/pdf/1903.07785
    img: cloze
    booktitle: arXiv
    booktitle-url: https://arxiv.org/abs/1903.07785
    code:
    venue: conference
    abstract: >
      We present a new approach for pretraining a bi-directional
      transformer model that provides significant performance
      gains across a variety of language understanding problems.
      Our model solves a cloze-style word reconstruction task,
      where each word is ablated and must be predicted given the
      rest of the text. Experiments demonstrate large performance
      gains on GLUE and new state of the art results on NER as
      well as constituency parsing benchmarks, consistent with
      the concurrently introduced BERT model. We also present a
      detailed analysis of a number of factors that contribute
      to effective pretraining, including data domain and size,
      model capacity, and variations on the cloze objective.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    title: "Mixture Models for Diverse Machine Translation: Tricks of the Trade"
    authors: Tianxiao Shen, Myle Ott, Michael Auli, Marc'Aurelio Ranzato
    doc-url: https://arxiv.org/pdf/1902.07816
    img: moe
    booktitle: Proc. of ICML
    booktitle-url: https://arxiv.org/abs/1902.07816
    code: https://github.com/pytorch/fairseq
    venue: conference
    abstract: >
      Mixture models trained via EM are among the simplest, most
      widely used and well understood latent variable models in the
      machine learning literature. Surprisingly, these models have
      been hardly explored in text generation applications such as
      machine translation. In principle, they provide a latent
      variable to control generation and produce a diverse set of
      hypotheses. In practice, however, mixture models are prone to
      degeneracies---often only one component gets trained or the
      latent variable is simply ignored. We find that disabling
      dropout noise in responsibility computation is critical to
      successful training. In addition, the design choices of
      parameterization, prior distribution, hard versus soft EM and
      online versus offline assignment can dramatically affect model
      performance. We develop an evaluation protocol to assess both
      quality and diversity of generations against multiple
      references, and provide an extensive empirical study of several
      mixture model variants. Our analysis shows that certain types of
      mixture models are more robust and offer the best trade-off
      between translation quality and diversity compared to
      variational models and diverse decoding approaches.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    authors: Dario Pavllo, Christoph Feichtenhofer, Michael Auli, David Grangier
    title: Modeling Human Motion with Quaternion-based Neural Networks
    doc-url: https://arxiv.org/pdf/1901.07677
    img: quaternet
    booktitle: IJCV
    booktitle-url: https://arxiv.org/abs/1901.07677
    venue: conference
    abstract: >
      Previous work on predicting or generating 3D human pose
      sequences regresses either joint rotations or joint positions.
      The former strategy is prone to error accumulation along
      the kinematic chain, as well as discontinuities when using
      Euler angles or exponential maps as parameterizations.
      The latter requires re-projection onto skeleton constraints
      to avoid bone stretching and invalid configurations.
      This work addresses both limitations. QuaterNet represents
      rotations with quaternions and our loss function performs
      forward kinematics on a skeleton to penalize absolute
      position errors instead of angle errors.
      We investigate both recurrent and convolutional architectures
      and evaluate on short-term prediction and long-term generation.
      For the latter, our approach is qualitatively judged as
      realistic as recent neural strategies from the graphics
      literature.
      Our experiments compare quaternions to Euler angles
      as well as exponential maps and show that only a
      very short context is required to make reliable future
      predictions.
      Finally, we show that the standard evaluation protocol
      for Human3. 6M produces high variance results and
      we propose a simple solution.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2019
    authors: Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, Michael Auli
    title: Pay Less Attention with Lightweight and Dynamic Convolutions
    doc-url: https://arxiv.org/pdf/1901.10430.pdf
    code: https://github.com/pytorch/fairseq
    img: dynamicconv
    booktitle: Proc. of ICLR
    booktitle-url: https://arxiv.org/abs/1901.10430
    venue: conference
    abstract: >
      Self-attention is a useful mechanism to build generative models
      for language and images.
      It determines the importance of context elements by comparing
      each element to the current time step. In this paper, we show
      that a very lightweight convolution can perform competitively
      to the best reported self-attention results.
      Next, we introduce dynamic convolutions which are simpler and
      more efficient than self-attention.
      We predict separate convolution kernels based solely on the
      current time-step in order to determine the importance of
      context elements.
      The number of operations required by this approach scales
      linearly in the input length, whereas self-attention is quadratic.
      Experiments on large-scale machine translation,
      language modeling and abstractive summarization show that
      dynamic convolutions improve over strong self-attention models.
      On the WMT'14 English-German test set dynamic convolutions
      achieve a new state of the art of 29.7 BLEU.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    authors: Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston
    title: 'Wizard of Wikipedia: Knowledge-Powered Conversational agents'
    doc-url: https://arxiv.org/pdf/1811.01241
    img: wizard
    booktitle: Proc. of ICLR
    booktitle-url: https://arxiv.org/abs/1811.01241
    venue: conference
    abstract: >
      In open-domain dialogue intelligent agents should exhibit
      the use of knowledge, however there are few convincing demonstrations
      of this to date.
      The most popular sequence to sequence models typically" generate
      and hope" generic utterances that can be memorized in the weights
      of the model when mapping from input utterance (s) to output,
      rather than employing recalled knowledge as context.
      Use of knowledge has so far proved difficult, in part because
      of the lack of a supervised learning benchmark task which exhibits
      knowledgeable open dialogue with clear grounding.
      To that end we collect and release a large dataset with conversations
      directly grounded with knowledge retrieved from Wikipedia.
      We then design architectures capable of retrieving knowledge,
      reading and conditioning on it, and finally generating natural
      responses.
      Our best performing dialogue models are able to conduct
      knowledgeable discussions on open-domain topics as evaluated
      by automatic metrics and human evaluations, while our
      new benchmark allows for measuring further improvements
      in this important research direction.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2019
    authors: Alexei Baevski, Michael Auli
    title: Adaptive Input Representations for Neural Language Modeling
    doc-url: https://arxiv.org/pdf/1809.10853
    img: adaptive
    booktitle: Proc. of ICLR
    code: https://github.com/pytorch/fairseq
    booktitle-url: https://arxiv.org/abs/1809.10853
    venue: conference
    abstract: >
      We introduce adaptive input representations for neural language
      modeling which extend the adaptive softmax of
      Grave et al. (2017) to input representations of variable capacity.
      There are several choices on how to factorize the input and
      output layers, and whether to model words, characters or
      sub-word units.
      We perform a systematic comparison of popular choices for
      a self-attentional architecture. Our experiments show that
      models equipped with adaptive embeddings are more than
      twice as fast to train than the popular character input CNN
      while having a lower number of parameters.
      We achieve a new state of the art on the WikiText-103
      benchmark of 20.51 perplexity, improving the next best
      known result by 8.7 perplexity. On the Billion word benchmark,
      we achieve a state of the art of 24.14 perplexity.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2018
    authors: Dario Pavllo, Christoph Feichtenhofer, David Grangier, Michael Auli
    title: 3D human pose estimation in video with temporal convolutions and semi-supervised training
    doc-url: https://arxiv.org/pdf/1811.11742
    code: https://github.com/facebookresearch/VideoPose3D
    img: videopose
    booktitle: Proc. of CVPR
    booktitle-url: https://arxiv.org/abs/1811.11742
    venue: conference
    abstract: >
      In this work, we demonstrate that 3D poses in video can
      be effectively estimated with a fully convolutional model
      based on dilated temporal convolutions over 2D keypoints.
      We also introduce back-projection, a simple and effective
      semi-supervised training method that leverages unlabeled
      video data.
      We start with predicted 2D keypoints for unlabeled video,
      then estimate 3D poses and finally back-project to
      the input 2D keypoints.
      In the supervised setting, our fully-convolutional model
      outperforms the previous best result from the literature
      by 6 mm mean per-joint position error on Human3. 6M,
      corresponding to an error reduction of 11%, and the model
      also shows significant improvements on HumanEva-I.
      Moreover, experiments with back-projection show that it
      comfortably outperforms previous state-of-the-art results
      in semi-supervised settings where labeled data is scarce.
      Code and models are available.
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2018
    authors: Sergey Edunov, Myle Ott, David Grangier, Michael Auli
    title: Understanding Back-Translation at Scale
    doc-url: https://arxiv.org/pdf/1808.09381
    code: https://github.com/pytorch/fairseq
    img: backtranslate
    booktitle: Proc. of EMNLP
    booktitle-url: https://arxiv.org/abs/1806.00187
    venue: conference
    abstract: >
      An effective method to improve neural machine translation
      with monolingual data is to augment the parallel training
      corpus with back-translations of target language sentences.
      This work broadens the understanding of back-translation
      and investigates a number of methods to generate synthetic
      source sentences. We find that in all but resource poor
      settings back-translations obtained via sampling or noised
      beam outputs are most effective. Our analysis shows that
      sampling or noisy synthetic data gives a much stronger
      training signal than data generated by beam or greedy search.
      We also compare how synthetic data compares to genuine bitext
      and study various domain effects. Finally, we scale to
      hundreds of millions of monolingual sentences and achieve
      a new state of the art of 35 BLEU on the
      WMT'14 English-German test set.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2018
    authors: Myle Ott, Sergey Edunov, David Grangier, Michael Auli
    title: Scaling Neural Machine Translation
    doc-url: https://arxiv.org/pdf/1806.00187
    code: https://github.com/pytorch/fairseq
    img: scaling
    booktitle: Proc. of WMT
    booktitle-url: https://arxiv.org/abs/1806.00187
    venue: conference
    abstract: >
      Sequence to sequence learning models still require several days
      to reach state of the art performance on large benchmark datasets
      using a single machine. This paper shows that reduced precision
      and large batch training can speedup training by nearly 5x on
      a single 8-GPU machine with careful tuning and implementation.
      On WMT'14 English-German translation, we match the accuracy
      of Vaswani et al. (2017) in under 5 hours when training
      on 8 GPUs and we obtain a new state of the art of 29.3 BLEU
      after training for 91 minutes on 128 GPUs.
      We further improve these results to 29.8 BLEU by training
      on the much larger Paracrawl dataset.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2018
    authors: Dario Pavllo, David Grangier, Michael Auli
    title: 'QuaterNet: A Quaternion-based Recurrent Model for Human Motion'
    doc-url: https://arxiv.org/pdf/1805.06485.pdf
    code: https://github.com/facebookresearch/QuaterNet
    img: quaternet
    booktitle: Proc. of BMVC
    booktitle-url: https://arxiv.org/abs/1805.06485
    venue: conference
    abstract: >
      Deep learning for predicting or generating 3D human pose sequences
      is an active research area. Previous work regresses either joint
      rotations or joint positions. The former strategy is prone to error
      accumulation along the kinematic chain, as well as discontinuities
      when using Euler angle or exponential map parameterizations.
      The latter requires re-projection onto skeleton constraints to avoid
      bone stretching and invalid configurations. This work addresses both
      limitations. Our recurrent network, QuaterNet, represents rotations
      with quaternions and our loss function performs forward kinematics
      on a skeleton to penalize absolute position errors instead of angle
      errors. On short-term predictions, QuaterNet improves the
      state-of-the-art quantitatively. For long-term generation, our
      approach is qualitatively judged as realistic as recent neural
      strategies from the graphics literature.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2018
    authors: Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato
    title: Analyzing Uncertainty in Neural Machine Translation
    doc-url: https://arxiv.org/pdf/1803.00047
    img: uncertainty
    booktitle: Proc. of ICML
    booktitle-url: https://arxiv.org/abs/1803.00047
    venue: conference
    abstract: >
      Machine translation is a popular test bed for research in neural
      sequence-to-sequence models but despite much recent research,
      there is still a lack of understanding of these models.
      Practitioners report performance degradation with large beams,
      the under-estimation of rare words and a lack of diversity
      in the final translations. Our study relates some of these issues
      to the inherent uncertainty of the task, due to the existence of
      multiple valid translations for a single source sentence, and
      to the extrinsic uncertainty caused by noisy training data.
      We propose tools and metrics to assess how uncertainty in the data
      is captured by the model distribution and how it affects search
      strategies that generate translations. Our results show that
      search works remarkably well but that the models tend to spread
      too much probability mass over the hypothesis space.
      Next, we propose tools to assess model calibration and show how
      to easily fix some shortcomings of current models. We release
      both code and multiple human reference translations for two
      popular benchmarks.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2018
    authors: Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato
    title: Classical Structured Prediction Losses for Sequence to Sequence Learning
    doc-url: https://arxiv.org/pdf/1711.04956
    img: classicseqlvl
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2018/
    venue: conference
    code: https://github.com/pytorch/fairseq/tree/classic_seqlevel
    abstract: >
      There has been much recent work on training neural attention models
      at the sequence-level using either reinforcement learning-style methods
      or by optimizing the beam. In this paper, we survey a range of
      classical objective functions that have been widely used to train
      linear models for structured prediction and apply them to neural
      sequence to sequence models. Our experiments show that these losses
      can perform surprisingly well by slightly outperforming beam search
      optimization in a like for like setup. We also report new state of
      the art results on both IWSLT 2014 German-English translation as well
      as Gigaword abstractive summarization.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2017
    authors: Angela Fan, David Grangier, Michael Auli
    title: Controllable Abstractive Summarization
    doc-url: https://arxiv.org/pdf/1711.05217
    img: controlabs
    booktitle: arXiv:1711.05217
    booktitle-url: https://arxiv.org/abs/1711.05217
    venue: conference
    abstract: >
      Current models for document summarization ignore user preferences
      such as the desired length, style or entities that the user has
      a preference for. We present a neural summarization model that
      enables users to specify such high level attributes in order
      to control the shape of the final summaries to better suit
      their needs. With user input, we show that our system can produce
      high quality summaries that are true to user preference.
      Without user input, we can set the control variables automatically
      and outperform comparable state of the art summarization systems
      despite the relative simplicity of our model.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2018
    authors: David Grangier, Michael Auli
    title: QuickEdit&#58; Editing Text &amp; Translations via Simple Delete Actions
    doc-url: https://arxiv.org/pdf/1711.04805
    img: quickedit
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2018/
    venue: conference
    abstract: >
      We propose a framework for computer-assisted text editing.
      It applies to translation post-editing and to paraphrasing and
      relies on very simple interactions. A human editor modifies a sentence
      by marking tokens they would like the system to change.
      Our model then generates a new sentence which reformulates
      the initial sentence by avoiding the words from the marked tokens.
      Our approach builds upon neural sequence-to-sequence modeling and
      introduces a neural network which takes as input a sentence along
      with deleted token markers. Our model is trained on translation
      bi-text by simulating post-edits. Our results on post-editing
      for machine translation and paraphrasing evaluate the performance
      of our approach. We show +11.4 BLEU with limited post-editing effort
      on the WMT-14 English-German translation task (25.2 to 36.6),
      which represents +5.9 BLEU over the post-editing baseline (30.7 to 36.6).
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2017
    authors: Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin
    title: Convolutional Sequence to Sequence Learning
    doc-url: https://arxiv.org/pdf/1705.03122
    img: fconv
    booktitle: Proc. of ICML
    booktitle-url:
    venue: conference
    code: https://github.com/facebookresearch/fairseq
    blog: https://engineering.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/
    abstract: >
      The prevalent approach to sequence to sequence learning maps
      an input sequence to a variable length output sequence via
      recurrent neural networks. We introduce an architecture based
      entirely on convolutional neural networks. Compared to
      recurrent models, computations over all elements can be fully
      parallelized during training and optimization is easier since
      the number of non-linearities is fixed and independent of the
      input length. Our use of gated linear units eases gradient
      propagation and we equip each decoder layer with a separate
      attention module. We outperform the accuracy of the deep
      LSTM setup of Wu et al. (2016) on both WMT'14 English-German and
      WMT'14 English-French translation at an order of magnitude
      faster speed, both on GPU and CPU.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2017
    authors: Yann N. Dauphin, Angela Fan, Michael Auli and David Grangier
    title: Language Modeling with Gated Convolutional Networks
    doc-url: papers/gcnn.pdf
    img: gcnn
    booktitle: Proc. of ICML
    booktitle-url:
    venue: conference
    abstract: >
      The pre-dominant approach to language modeling to date
      is based on recurrent neural networks.
      In this paper we present a convolutional approach
      to language modeling. We introduce a novel
      gating mechanism that eases gradient propagation
      and which performs better than the LSTM-style
      gating of Oord et al. (2016b) despite being
      simpler. We achieve a new state of the art on
      WikiText-103 as well as a new best single-GPU
      result on the Google Billion Word benchmark.
      In settings where latency is important, our model
      achieves an order of magnitude speed-up compared
      to a recurrent baseline since computation
      can be parallelized over time. To our knowledge,
      this is the first time a non-recurrent approach outperforms
      strong recurrent models on these tasks.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2017
    authors: Jonas Gehring, Michael Auli, David Grangier, Yann N. Dauphin
    title: A Convolutional Encoder Model for Neural Machine Translation
    doc-url: papers/convenc.pdf
    img: convenc
    booktitle: Proc. of ACL
    booktitle-url:
    venue: conference
    abstract: >
      The prevalent approach to neural machine translation
      relies on bi-directional LSTMs to encode the source sentence.
      In this paper we present a faster and conceptually simpler
      architecture based on a succession of convolutional layers.
      This allows to encode the entire source sentence simultaneously
      compared to recurrent networks for which computation is
      constrained by temporal dependencies.
      We achieve a new state-of-the-art on WMT'16 English-Romanian
      translation and outperform several recently published
      results on the WMT'15 English-German task.
      We also achieve almost the same accuracy as a very deep LSTM setup
      on WMT'14 English-French translation.
      Our convolutional encoder speeds up CPU decoding by more than
      two times at the same or higher accuracy as a strong
      bi-directional LSTM baseline.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2016
    authors: Roman Novak, Michael Auli, David Grangier
    title: Iterative Refinement for Machine Translation
    doc-url: papers/refine.pdf
    img: refine
    booktitle: arXiv:1610.06602
    booktitle-url:
    venue: conference
    abstract: >
      Existing machine translation decoding algorithms generate
      translations in a strictly monotonic fashion and never revisit
      previous decisions.
      As a result, earlier mistakes cannot be corrected at a later stage.
      In this paper, we present a translation scheme that starts
      from an initial guess and then makes iterative improvements
      that may revisit previous decisions.
      We parameterize our model as a convolutional neural network
      that predicts discrete substitutions to an existing translation
      based on an attention mechanism over both the source sentence
      as well as the current translation output.
      By making less than one modification per sentence,
      we improve the output of a phrase-based translation system
      by up to 0.4 BLEU on WMT15 German-English translation.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2016
    authors: Gurvan L'Hostis, David Grangier, Michael Auli
    title: Vocabulary Selection Strategies for Neural Machine Translation
    doc-url: papers/vocabsel.pdf
    img: vocabsel
    booktitle: arXiv:1610.00072
    booktitle-url:
    venue: conference
    abstract: >
      Classical translation models constrain the space of possible outputs
      by selecting a subset of translation rules based on the input sentence.
      Recent work on improving the efficiency of neural translation models
      adopted a similar strategy by restricting the output vocabulary
      to a subset of likely candidates given the source.
      In this paper we experiment with context and embedding-based
      selection methods and extend previous work by examining speed and
      accuracy trade-offs in more detail.
      We show that decoding time on CPUs can be reduced by up to 90% and
      training time by 25% on the WMT15 English-German and
      WMT16 English-Romanian tasks at the same or only negligible
      change in accuracy.
      This brings the time to decode with a state of the art
      neural translation system to just over 140 msec per sentence
      on a single CPU core for English-German.

  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2016
    authors: Remi Lebret, David Grangier, and Michael Auli
    title: Neural Text Generation from Structured Data with Application to the Biography Domain
    doc-url: papers/emnlp2016_biogen.pdf
    img: emnlp2016_biogen
    booktitle: Proc. of EMNLP
    booktitle-url: http://emnlp2016.net
    data: http://github.com/DavidGrangier/wikipedia-biography-dataset
    venue: conference
    abstract: >
      This paper introduces a neural model for concept-to-text
      generation that scales to large, rich domains.
      It generates biographical sentences from fact tables on
      a new dataset of biographies from Wikipedia.
      This set is an order of magnitude larger than existing
      resources with over 700k samples and a 400k vocabulary.
      Our model builds on conditional neural language models
      for text generation.
      To deal with the large vocabulary, we extend these models
      to mix a fixed vocabulary with copy actions that transfer
      sample-specific words from the input database to
      the generated output sentence.
      To deal with structured data, we allow the model to
      embed words differently depending on the data fields
      in which they occur.
      Our neural model significantly outperforms a Templated
      Kneser-Ney language model by nearly 15 BLEU.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2016
    authors: Joel Legrand, Michael Auli, and Ronan Collobert
    title: Neural Network-based Word Alignment through Score Aggregation
    doc-url: papers/wmt2016_align.pdf
    img: wmt2016_align
    booktitle: Proc. of WMT
    booktitle-url: http://statmt.org/wmt16
    venue: conference
    abstract: >
      We present a simple neural network for word alignment
      that builds source and target word window representations
      to compute alignment scores for sentence pairs.
      To enable unsupervised training, we use an aggregation
      operation that summarizes the alignment scores for a
      given target word.
      A soft-margin objective increases scores for true target
      words while decreasing scores for target words that are
      not present.
      Compared to the popular Fast Align model, our approach
      improves alignment accuracy by 7 AER on English-Czech,
      by 6 AER on Romanian-English and by 1.7 AER on
      English-French alignment.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2016
    authors: Wenlin Chen, David Grangier, Michael Auli
    title: Strategies for Training Large Vocabulary Neural Language Models
    doc-url: papers/acl2016_large_vocab_lm.pdf
    img: acl2016_large_vocab_lm
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2016.org
    venue: conference
    abstract: >
      Training neural network language models over large
      vocabularies is computationally costly compared to
      count-based models such as Kneser-Ney.
      We present a systematic comparison of neural strategies
      to represent and train large vocabularies, including
      softmax, hierarchical softmax, target sampling, noise
      contrastive estimation and self normalization.
      We extend self normalization to be a proper estimator
      of likelihood and introduce an efficient variant of
      softmax.
      We evaluate each method on three popular benchmarks,
      examining performance on rare words, the speed/accuracy
      trade-off and complementarity to Kneser-Ney.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2016
    authors: Wenduan Xu, Michael Auli, and Stephen Clark
    title: Expected F-Measure Training for Shift-Reduce Parsing with Recurrent Neural Networks
    doc-url: papers/naacl2016_xf1_ccg.pdf
    img: naacl2016_xf1_ccg
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2016
    venue: conference
    abstract: >
      We present expected F-measure training for shift-reduce
      parsing with RNNs, which enables the learning of a global
      parsing model optimized for sentence-level F1.
      We apply the model to CCG parsing, where it improves
      over a strong greedy RNN baseline, by 1.47% F1, yielding
      state-of-the-art results for shift-reduce CCG parsing.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2016
    authors: Sumit Chopra, Michael Auli, and Alexander M. Rush
    title: Abstractive Sentence Summarization with Attentive Recurrent Neural Networks
    doc-url: papers/naacl2016_summary_rnn.pdf
    img: naacl2016_summary_rnn
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2016
    venue: conference
    abstract: >
      Abstractive Sentence Summarization generates a shorter
      version of a given sentence while attempting to preserve
      its meaning.
      We introduce a conditional recurrent neural network (RNN)
      which generates a summary of an input sentence.
      The conditioning is provided by a novel convolutional
      attention-based encoder which ensures that the decoder
      focuses on the appropriate input words at each step of
      generation.
      Our model relies only on learned features and is easy to
      train in an end-to-end fashion on large data sets.
      Our experiments show that the model significantly outperforms
      the recently proposed state-of-the-art method on the
      Gigaword corpus while performing competitively on the
      DUC-2004 shared task.
  -
    layout: paper
    paper-type: inproceedings
    selected: true
    year: 2016
    authors: Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba
    title: Sequence Level Training with Recurrent Neural Networks
    doc-url: papers/iclr2016_mixer.pdf
    img: iclr2016_mixer
    code: http://github.com/facebookresearch/MIXER
    booktitle: Proc. of ICLR
    booktitle-url: http://www.iclr.cc/doku.php?id=iclr2016:main
    venue: conference
    abstract: >
      Many natural language processing applications use language
      models to generate text.
      These models are typically trained to predict the next word
      in a sequence, given the previous words and some context
      such as an image.
      However, at test time the model is expected to generate the
      entire sequence from scratch.
      This discrepancy makes generation brittle, as errors may
      accumulate along the way.
      We address this issue by proposing a novel sequence level
      training algorithm that directly optimizes the metric used
      at test time, such as BLEU or ROUGE.
      On three different tasks, our approach outperforms several
      strong baselines for greedy generation.
      The method is also competitive when these baselines employ
      beam search, while being several times faster.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2015
    authors: Wenduan Xu, Michael Auli, and Stephen Clark
    title: CCG Supertagging with a Recurrent Neural Network
    doc-url: papers/super-rnn.pdf
    img: super-rnn
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2015.org
    venue: conference
    abstract: >
      Recent work on supertagging using a feed-forward neural network
      achieved significant improvements for CCG supertagging and
      parsing (Lewis and Steedman, 2014).
      However, their architecture is limited to considering local
      context and does not naturally model sequences of arbitrary
      length.
      In this paper, we show directly capturing sequence information
      using a recurrent neural network leads to further supertagging
      (up to 1.9%) and parsing accuracy improvements (up to 1% F1) on
      CCG-Bank, Wikipedia and biomedical text.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2015
    authors: Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao and Bill Dolan
    title: deltaBLEU&#58; A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets
    doc-url: papers/delta-bleu.pdf
    img: delta-bleu
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2015.org
    venue: conference
    abstract: >
      We introduce Discriminative BLEU (deltaBLEU), a novel metric
      for intrinsic evaluation of generated text in tasks that admit
      a diverse range of possible outputs.
      Reference strings are scored for quality by human raters on a
      scale of [-1.0, +1.0] to weight multi-reference BLEU.
      In tasks involving generation of conversational responses,
      deltaBLEU correlates reasonably with human judgments and outperforms
      sentence-level and IBM BLEU in terms of both Spearman's rho and
      Kendall's tau.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2015
    authors: Kai Zhao, Hany Hassan, and Michael Auli
    title: Learning Translation Models from Monolingual Continuous Representations
    doc-url: papers/dist_phrase_learn.pdf
    img: dist_phrase_learn
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2015
    venue: conference
    abstract: >
      The availability of parallel corpora limits the further success
      of statistical machine translation since infrequent phrases lack
      appropriate translation rules.
      Existing approaches that leverage the huge monolingual corpora
      usually need specially designed statistics for phrases, which
      take a long time to calculate.
      We propose a simple and fast translation model learning method
      that directly hallucinates translation rules for infrequent
      phrases from semantically similar neighbors, using
      continuous phrasal representations based on widely available
      word vectors.
      We also investigate approximated nearest neighbors query to
      further speed up search for semantically similar neighbors
      in continuous representation space.
      Experiments show that adding translation rules for infrequent phrases
      generated by our algorithm can improve accuracy by up to 1.5 BLEU.
      Compared to existing methods, our approach generates better
      translation rules, and is faster.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2015
    authors: Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Meg Mitchell, Jianfeng Gao, Bill Dolan, and Jian-Yun Nie
    title: A Neural Network Approach to Context-Sensitive Generation of Conversational Responses
    doc-url: papers/chitchat.pdf
    img: chitchat
    booktitle: Proc. of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2015
    venue: conference
    abstract: >
      We present a novel response generation system that can be
      trained from end to end on large quantities of unstructured
      Twitter conversations.
      A neural network architecture is used to address sparsity issues
      that arise when integrating context information into classical
      statistical models, allowing the system to take into account
      previous dialog utterances.
      Our dynamic-context generative models show consistent gains over
      both context-sensitive and non-context-sensitive Machine Translation
      and Information Retrieval baselines.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2014
    authors: Michael Auli, Michel Galley, and Jianfeng Gao
    title: Large Scale Expected BLEU Training of Phrase-based Reordering Models
    doc-url: papers/expbleu-reorder.pdf
    img: expbleu-reorder
    booktitle: Proc. of EMNLP
    booktitle-url: http://emnlp2014.org/
    venue: conference
    abstract: >
      Recent work by Cherry (2013) has shown that directly
      optimizing phrase-based reordering models towards BLEU
      can lead to significant gains. Their approach is limited
      to small training sets of a few thousand sentences and
      a similar number of sparse features. We show how the
      expected BLEU objective allows us to train a simple
      linear discriminative reordering model with millions
      of sparse features on hundreds of thousands of sentences,
      each of which results in significant improvements.
      A comparison to likelihood training demonstrates that
      expected BLEU is vastly more effective. Our best results
      improve a hierarchical lexicalized reordering baseline
      by up to 2.0 BLEU in a single-reference setting on a
      French-English WMT 2012 setup.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2014
    authors: Michael Auli and Jianfeng Gao
    title: Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models
    doc-url: papers/expbleu-rnn.pdf
    img: expbleu-rnn
    booktitle: Proc. of ACL
    booktitle-url: http://acl2014.org
    venue: conference
    abstract: >
      Neural network language models are often trained by optimizing likelihood, but
      we would prefer to optimize for a task specific metric, such as BLEU for machine
      translation.
      We show how a recurrent neural network language model can be optimized towards
      an expected BLEU loss instead of the usual cross-entropy criterion.
      Furthermore, we tackle the issue of directly integrating a recurrent
      network into first-pass decoding under an efficient approximation.
      Our best results improve a phrase-based statistical machine translation
      system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the
      expected BLEU objective improves over a cross-entropy trained model by up to
      0.6 BLEU in a single reference setup.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2014
    authors: Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao
    title: Minimum Translation Modeling with Recurrent Neural Networks
    doc-url: papers/mtu-rnn.pdf
    img: mtu-rnn
    booktitle: Proc. of EACL
    booktitle-url: http://eacl2014.org
    venue: conference
    abstract: >
      We introduce recurrent neural network-based Minimum Translation Unit (MTU)
      models which make predictions based on an unbounded history of previous
      bilingual contexts.
      Traditional back-off n-gram models suffer under the sparse nature of MTUs which
      makes estimation of high-order sequence models challenging.
      We tackle the sparsity problem by modeling MTUs both as bags-of-words and as
      a sequence of individual source and target words.
      Our best results improve the output of a phrase-based statistical machine
      translation system trained on WMT 2012 French-English data by up to 1.5 BLEU,
      and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2013
    authors: Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig
    title: Joint Language and Translation Modeling with Recurrent Neural Networks
    doc-url: papers/rnn-joint-lm-tm.pdf
    img: rnn-joint-lm-tm
    booktitle: Proc. of EMNLP
    booktitle-url: http://hum.csse.unimelb.edu.au/emnlp2013
    venue: conference
    abstract: >
      We present a joint language and translation model based on a
      recurrent neural network which predicts target words based on
      an unbounded history of both source and target words. The weaker
      independence asssumptions of this model result in a vastly larger
      search space compared to related feed forward-based language or
      translation models. We tackle this issue with a new lattice rescoring
      algorithm and demonstrate its effectiveness empirically. Our joint
      model builds on a well known recurrent neural network language model
      (Mikolov, 2012) augmented by a layer of additional inputs from the
      source language. We show competitive accuracy compared to the
      traditional channel model features. Our best results improve the output
      of a system trained on WMT 2012 French-English data by up to 1.5 BLEU,
      and by 1.1 BLEU on average across several test sets.
  -
    layout: paper
    paper-type: phd-thesis
    selected: false
    year: 2012
    title: Integrated Supertagging and Parsing
    institution: University of Edinburgh
    doc-url: papers/michael.auli.thesis.pdf
    img: thesis2012
    abstract: >
      <p>Parsing is the task of assigning syntactic or semantic structure to a natural language sentence. This thesis focuses on syntactic parsing with Combinatory Categorial Grammar (CCG; Steedman 2000). CCG allows incremental processing, which is essential for speech recognition and some machine translation models, and it can build semantic structure in tandem with syntactic parsing. Supertagging solves a subset of the parsing task by assigning lexical types to words in a sentence using a sequence model. It has emerged as a way to improve the efficiency of full CCG parsing (Clark and Curran, 2007) by reducing the parser's search space. This has been very successful and it is the central theme of this thesis.

      <p>We begin by an analysis of how efficiency is being traded for accuracy in supertagging. Pruning the search space by supertagging is inherently approximate and to contrast this we include A* in our analysis, a classic exact search technique.Interestingly, we find that combining the two methods improves efficiency but we also demonstrate that excessive pruning by a supertagger significantly lowers the upper bound on accuracy of a CCG parser.

      <p>Inspired by this analysis, we design a single integrated model with both supertagging and parsing features, rather than separating them into distinctmodels chained together in a pipeline. To overcome the resulting complexity, we experiment with both loopy belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem.

      <p>Finally, we address training the integrated model. We adopt the idea of optimising directly for a task-specific metric such as is common in other areas like statistical machine translation. We demonstrate how a novel dynamic programming algorithm enables us to optimise for F-measure, our task-specific evaluation metric, and experiment with approximations, which prove to be excellent substitutions.

      <p>Each of the presented methods improves over the state-of-the-art in CCG parsing. Moreover, the improvements are additive, achieving a labelled/unlabelled dependency F-measure on CCGbank of 89.3%/94.0% with gold part-of-speech tags, and 87.2%/92.8% with automatic part-of-speech tags, the best reported results for this task to date. Our techniques are general and we expect them to apply to other parsing problems, including lexicalised tree adjoining grammar and context-free grammar parsing.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: Training a Log-Linear Parser with Loss Functions via Softmax-Margin
    doc-url: papers/softmax-ccg.pdf
    img: softmax-ccg
    booktitle: Proc. of EMNLP
    booktitle-url: http://conferences.inf.ed.ac.uk/emnlp2011/
    venue: conference
    abstract: >
      Log-linear parsing models are often trained by optimizing
      likelihood, but we would prefer to optimize for a task-specific metric like F-measure.
      Softmax-margin is a convex objective for such models that minimizes a bound on
      expected risk for a given loss function, but its naÃ¯ve application requires the loss
      to decompose over the predicted structure, which is not true of F-measure.
      We use softmax-margin to optimize a log-linear CCG parser for a variety of loss functions, and
      demonstrate a novel dynamic programming algorithm that enables us to use it with
      F-measure, leading to substantial gains in accuracy on CCGBank.  When we embed our
      loss-trained parser into a larger model that includes supertagging features
      incorporated via belief propagation, we obtain further improvements and achieve
      a labelled/unlabelled dependency
      F-measure of 89.3%/94.0% on gold part-of-speech tags,
      and 87.2%/92.8% on automatic part-of-speech
      tags, the best reported results for this task.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing
    doc-url: papers/lbp-vs-dd.pdf
    img: lbp-vs-dd
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2011.org/
    venue: conference
    abstract: >
      Via an oracle experiment, we show that the upper bound on accuracy of a CCG
      parser is significantly lowered when its search space is pruned using a
      supertagger, though the supertagger also prunes many bad parses.  Inspired by
      this analysis, we design a single model with both supertagging and parsing
      features, rather than separating them into distinct models chained together
      in a pipeline.  To overcome the resulting increase in complexity, we
      experiment with both belief propagation and dual decomposition approaches to
      inference, the first empirical comparison of these algorithms that we are
      aware of on a structured natural language processing problem.  On CCGbank we
      achieve a labelled dependency F-measure of 88.8% on gold POS tags, and
      86.7% on automatic part-of-speech tags, the best reported results for this
      task.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: Efficient CCG Parsing&#58; A* versus Adaptive Supertagging
    doc-url: papers/astar-ccg.pdf
    img: astar-ccg
    booktitle: Proc. of ACL
    booktitle-url: http://www.acl2011.org/
    venue: conference
    abstract: >
      We present a systematic comparison and combination of two orthogonal techniques for efficient
      parsing of Combinatory Categorial Grammar (CCG).  First we consider adaptive supertagging, a
      widely used approximate search technique that prunes most lexical categories from the parser's
      search space using a separate sequence model.  Next we consider several variants on A*, a
      classic exact search technique which to our knowledge has not been applied to more expressive
      grammar formalisms like CCG.  In addition to standard hardware-independent measures of parser
      effort we also present what we believe is the first evaluation of A* parsing on the more
      realistic but more stringent metric of CPU time.  By itself, A* substantially reduces parser
      effort as measured by the number of edges considered during parsing, but we show that for CCG
      this does not always correspond to improvements in CPU time over a CKY baseline.  Combining A*
      with adaptive supertagging decreases CPU time by 15% for our best model.
  -
    layout: paper
    paper-type: firstyearreport
    selected: false
    year: 2009
    institution: University of Edinburgh
    title: CCG-based Models for Statistical Machine Translation
    doc-url: papers/first-year-report.pdf
    img: first-year-report
    abstract: >
      <p>The arguably best performing statistical machine translation systems are based
      on context-free formalisms or weakly equivalent ones. These models usually use
      a synchronous version of a context-free grammar (SCFG) which we argue is too
      rigid for the highly ambiguous task of human language translation. This is
      exacerbated by the fact that the imperfect methods available for aligning
      parallel texts make extracting an efficient grammar very hard. As a result,
      the context-free grammars extracted are usually very large in size after
      having already been restricted through a variety of constraints.

      <p>We propose to use Combinatorial Categorial Grammar (CCG) for machine translation
      models. CCG is a lexicalized, mildly-context-sensitive formalism which
      is very well suited to capture long-distance dependencies that are not
      addressed very well by most current models. We believe that CCG is very well
      suited for the task of machine translation due to its ability to represent
      non-constituents in a syntactic way which frequently occur in parallel texts
      as well as its high derivational flexibility. This allows us to use some of
      the advantages of non-syntactic phrase-based approaches within a syntactic
      framework such as a relatively small grammar size compared to
      context-free-based machine translation grammars.

      <p>A number of models leveraging the advantages of CCG are possible, however, our
      principal goal is to develop a string-to-tree based model which projects CCG
      on the target side of a synchronous grammar. We intend to apply the vast
      progress made in monolingual CCG parsing to machine translation. Additionally,
      we propose to extend CCG to a synchronous grammar (SCCG) as it has been done
      for other related formalisms such as tree adjoining grammars. We hope that a
      SCCG may provide similar derivational flexibility to monolingual CCG which may
      result in a better model for translational equivalence.
  -
    layout: paper
    paper-type: inproceedings
    selected: false
    year: 2009
    authors: Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn
    title: A Systematic Analysis of Translation Model Search Spaces
    doc-url: http://www.aclweb.org/anthology/W/W09/W09-0437.pdf
    booktitle: Proc. of WMT
    booktitle-url: http://www.statmt.org/wmt09/
    img: wmt2009
    venue: workshop
    abstract: >
      Translation systems are complex, and most metrics do little to pinpoint
      causes of error or isolate system differences.  We use a simple technique
      to discover induction errors, which occur when good translations are
      absent from model search spaces.  Our results show that a common pruning
      heuristic drastically increases induction error, and also strongly suggest
      that the search spaces of phrase-based and hierarchical phrase-based
      models are highly overlapping despite the well known structural
      differences.
