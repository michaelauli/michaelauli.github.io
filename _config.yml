exclude: ['README.md']
timezone: US/Pacific
talks:
  -
    title: Learning to translate with neural networks
    detail: Talk at Facebook, Google, Amazon and the University of Washington, 2014.
    url: "talks/facebook-rnn-translate.pdf"
    img: "facebook-rnn-translate.jpg"
  -
    title: Integrated Parsing and Tagging
    detail: Talk at Carnegie Mellon University, Johns Hopkins University, BBN Technologies, IBM Research and Microsoft Research, 2011.
    url: "talks/parsing-tagging-talk.pdf"
    img: "parsing-tagging-talk.jpg"
papers:
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2017
    authors: Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin
    title: Convolutional Sequence to Sequence Learning
    doc-url: https://arxiv.org/pdf/1705.03122.pdf
    img: fconv
    booktitle: arXiv:1705.03122
    booktitle-url:
    venue: conference
    code: https://github.com/facebookresearch/fairseq
    abstract: >
      The prevalent approach to sequence to sequence learning maps
      an input sequence to a variable length output sequence via
      recurrent neural networks. We introduce an architecture based
      entirely on convolutional neural networks. Compared to
      recurrent models, computations over all elements can be fully
      parallelized during training and optimization is easier since
      the number of non-linearities is fixed and independent of the
      input length. Our use of gated linear units eases gradient
      propagation and we equip each decoder layer with a separate
      attention module. We outperform the accuracy of the deep
      LSTM setup of Wu et al. (2016) on both WMT'14 English-German and
      WMT'14 English-French translation at an order of magnitude
      faster speed, both on GPU and CPU.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Yann N. Dauphin, Angela Fan, Michael Auli and David Grangier
    title: Language Modeling with Gated Convolutional Networks
    doc-url: papers/gcnn.pdf
    img: gcnn
    booktitle: arXiv:1612.08083
    booktitle-url:
    venue: conference
    abstract: >
      The pre-dominant approach to language modeling to date
      is based on recurrent neural networks.
      In this paper we present a convolutional approach
      to language modeling. We introduce a novel
      gating mechanism that eases gradient propagation
      and which performs better than the LSTM-style
      gating of Oord et al. (2016b) despite being
      simpler. We achieve a new state of the art on
      WikiText-103 as well as a new best single-GPU
      result on the Google Billion Word benchmark.
      In settings where latency is important, our model
      achieves an order of magnitude speed-up compared
      to a recurrent baseline since computation
      can be parallelized over time. To our knowledge,
      this is the first time a non-recurrent approach outperforms
      strong recurrent models on these tasks.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Jonas Gehring, Michael Auli, David Grangier, Yann N. Dauphin
    title: A Convolutional Encoder Model for Neural Machine Translation
    doc-url: papers/convenc.pdf
    img: convenc
    booktitle: arXiv:1611.02344
    booktitle-url:
    venue: conference
    abstract: >
      The prevalent approach to neural machine translation
      relies on bi-directional LSTMs to encode the source sentence.
      In this paper we present a faster and conceptually simpler
      architecture based on a succession of convolutional layers.
      This allows to encode the entire source sentence simultaneously
      compared to recurrent networks for which computation is
      constrained by temporal dependencies.
      We achieve a new state-of-the-art on WMT'16 English-Romanian
      translation and outperform several recently published
      results on the WMT'15 English-German task.
      We also achieve almost the same accuracy as a very deep LSTM setup
      on WMT'14 English-French translation.
      Our convolutional encoder speeds up CPU decoding by more than
      two times at the same or higher accuracy as a strong
      bi-directional LSTM baseline.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Roman Novak, Michael Auli, David Grangier
    title: Iterative Refinement for Machine Translation
    doc-url: papers/refine.pdf
    img: refine
    booktitle: arXiv:1610.06602
    booktitle-url:
    venue: conference
    abstract: >
      Existing machine translation decoding algorithms generate
      translations in a strictly monotonic fashion and never revisit
      previous decisions.
      As a result, earlier mistakes cannot be corrected at a later stage.
      In this paper, we present a translation scheme that starts
      from an initial guess and then makes iterative improvements
      that may revisit previous decisions.
      We parameterize our model as a convolutional neural network
      that predicts discrete substitutions to an existing translation
      based on an attention mechanism over both the source sentence
      as well as the current translation output.
      By making less than one modification per sentence,
      we improve the output of a phrase-based translation system
      by up to 0.4 BLEU on WMT15 German-English translation.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Gurvan L'Hostis, David Grangier, Michael Auli
    title: Vocabulary Selection Strategies for Neural Machine Translation
    doc-url: papers/vocabsel.pdf
    img: vocabsel
    booktitle: arXiv:1610.00072
    booktitle-url:
    venue: conference
    abstract: >
      Classical translation models constrain the space of possible outputs
      by selecting a subset of translation rules based on the input sentence.
      Recent work on improving the efficiency of neural translation models
      adopted a similar strategy by restricting the output vocabulary
      to a subset of likely candidates given the source.
      In this paper we experiment with context and embedding-based
      selection methods and extend previous work by examining speed and
      accuracy trade-offs in more detail.
      We show that decoding time on CPUs can be reduced by up to 90% and
      training time by 25% on the WMT15 English-German and
      WMT16 English-Romanian tasks at the same or only negligible
      change in accuracy.
      This brings the time to decode with a state of the art
      neural translation system to just over 140 msec per sentence
      on a single CPU core for English-German.

  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Remi Lebret, David Grangier, and Michael Auli
    title: Neural Text Generation from Structured Data with Application to the Biography Domain
    doc-url: papers/emnlp2016_biogen.pdf
    img: emnlp2016_biogen
    booktitle: Proceedings of EMNLP
    booktitle-url: http://emnlp2016.net
    data: http://github.com/DavidGrangier/wikipedia-biography-dataset
    venue: conference
    abstract: >
      This paper introduces a neural model for concept-to-text
      generation that scales to large, rich domains.
      It generates biographical sentences from fact tables on
      a new dataset of biographies from Wikipedia.
      This set is an order of magnitude larger than existing
      resources with over 700k samples and a 400k vocabulary.
      Our model builds on conditional neural language models
      for text generation.
      To deal with the large vocabulary, we extend these models
      to mix a fixed vocabulary with copy actions that transfer
      sample-specific words from the input database to
      the generated output sentence.
      To deal with structured data, we allow the model to
      embed words differently depending on the data fields
      in which they occur.
      Our neural model significantly outperforms a Templated
      Kneser-Ney language model by nearly 15 BLEU.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Joel Legrand, Michael Auli, and Ronan Collobert
    title: Neural Network-based Word Alignment through Score Aggregation
    doc-url: papers/wmt2016_align.pdf
    img: wmt2016_align
    booktitle: Proceedings of WMT
    booktitle-url: http://statmt.org/wmt16
    venue: conference
    abstract: >
      We present a simple neural network for word alignment
      that builds source and target word window representations
      to compute alignment scores for sentence pairs.
      To enable unsupervised training, we use an aggregation
      operation that summarizes the alignment scores for a
      given target word.
      A soft-margin objective increases scores for true target
      words while decreasing scores for target words that are
      not present.
      Compared to the popular Fast Align model, our approach
      improves alignment accuracy by 7 AER on English-Czech,
      by 6 AER on Romanian-English and by 1.7 AER on
      English-French alignment.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Wenlin Chen, David Grangier, Michael Auli
    title: Strategies for Training Large Vocabulary Neural Language Models
    doc-url: papers/acl2016_large_vocab_lm.pdf
    img: acl2016_large_vocab_lm
    booktitle: Proceedings of ACL
    booktitle-url: http://www.acl2016.org
    venue: conference
    abstract: >
      Training neural network language models over large
      vocabularies is computationally costly compared to
      count-based models such as Kneser-Ney.
      We present a systematic comparison of neural strategies
      to represent and train large vocabularies, including
      softmax, hierarchical softmax, target sampling, noise
      contrastive estimation and self normalization.
      We extend self normalization to be a proper estimator
      of likelihood and introduce an efficient variant of
      softmax.
      We evaluate each method on three popular benchmarks,
      examining performance on rare words, the speed/accuracy
      trade-off and complementarity to Kneser-Ney.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Wenduan Xu, Michael Auli, and Stephen Clark
    title: Expected F-Measure Training for Shift-Reduce Parsing with Recurrent Neural Networks
    doc-url: papers/naacl2016_xf1_ccg.pdf
    img: naacl2016_xf1_ccg
    booktitle: Proceedings of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2016
    venue: conference
    abstract: >
      We present expected F-measure training for shift-reduce
      parsing with RNNs, which enables the learning of a global
      parsing model optimized for sentence-level F1.
      We apply the model to CCG parsing, where it improves
      over a strong greedy RNN baseline, by 1.47% F1, yielding
      state-of-the-art results for shift-reduce CCG parsing.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Sumit Chopra, Michael Auli, and Alexander M. Rush
    title: Abstractive Sentence Summarization with Attentive Recurrent Neural Networks
    doc-url: papers/naacl2016_summary_rnn.pdf
    img: naacl2016_summary_rnn
    booktitle: Proceedings of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2016
    venue: conference
    abstract: >
      Abstractive Sentence Summarization generates a shorter
      version of a given sentence while attempting to preserve
      its meaning.
      We introduce a conditional recurrent neural network (RNN)
      which generates a summary of an input sentence.
      The conditioning is provided by a novel convolutional
      attention-based encoder which ensures that the decoder
      focuses on the appropriate input words at each step of
      generation.
      Our model relies only on learned features and is easy to
      train in an end-to-end fashion on large data sets.
      Our experiments show that the model significantly outperforms
      the recently proposed state-of-the-art method on the
      Gigaword corpus while performing competitively on the
      DUC-2004 shared task.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2016
    authors: Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba
    title: Sequence Level Training with Recurrent Neural Networks
    doc-url: papers/iclr2016_mixer.pdf
    img: iclr2016_mixer
    code: http://github.com/facebookresearch/MIXER
    booktitle: Proceedings of ICLR
    booktitle-url: http://www.iclr.cc/doku.php?id=iclr2016:main
    venue: conference
    abstract: >
      Many natural language processing applications use language
      models to generate text.
      These models are typically trained to predict the next word
      in a sequence, given the previous words and some context
      such as an image.
      However, at test time the model is expected to generate the
      entire sequence from scratch.
      This discrepancy makes generation brittle, as errors may
      accumulate along the way.
      We address this issue by proposing a novel sequence level
      training algorithm that directly optimizes the metric used
      at test time, such as BLEU or ROUGE.
      On three different tasks, our approach outperforms several
      strong baselines for greedy generation.
      The method is also competitive when these baselines employ
      beam search, while being several times faster.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2015
    authors: Wenduan Xu, Michael Auli, and Stephen Clark
    title: CCG Supertagging with a Recurrent Neural Network
    doc-url: papers/super-rnn.pdf
    img: super-rnn
    booktitle: Proceedings of ACL
    booktitle-url: http://www.acl2015.org
    venue: conference
    abstract: >
      Recent work on supertagging using a feed-forward neural network
      achieved significant improvements for CCG supertagging and
      parsing (Lewis and Steedman, 2014).
      However, their architecture is limited to considering local
      context and does not naturally model sequences of arbitrary
      length.
      In this paper, we show directly capturing sequence information
      using a recurrent neural network leads to further supertagging
      (up to 1.9%) and parsing accuracy improvements (up to 1% F1) on
      CCG-Bank, Wikipedia and biomedical text.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2015
    authors: Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao and Bill Dolan
    title: deltaBLEU&#58; A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets
    doc-url: papers/delta-bleu.pdf
    img: delta-bleu
    booktitle: Proceedings of ACL
    booktitle-url: http://www.acl2015.org
    venue: conference
    abstract: >
      We introduce Discriminative BLEU (deltaBLEU), a novel metric
      for intrinsic evaluation of generated text in tasks that admit
      a diverse range of possible outputs.
      Reference strings are scored for quality by human raters on a
      scale of [-1.0, +1.0] to weight multi-reference BLEU.
      In tasks involving generation of conversational responses,
      deltaBLEU correlates reasonably with human judgments and outperforms
      sentence-level and IBM BLEU in terms of both Spearman's rho and
      Kendall's tau.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2015
    authors: Kai Zhao, Hany Hassan, and Michael Auli
    title: Learning Translation Models from Monolingual Continuous Representations
    doc-url: papers/dist_phrase_learn.pdf
    img: dist_phrase_learn
    booktitle: Proceedings of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2015
    venue: conference
    abstract: >
      The availability of parallel corpora limits the further success
      of statistical machine translation since infrequent phrases lack
      appropriate translation rules.
      Existing approaches that leverage the huge monolingual corpora
      usually need specially designed statistics for phrases, which
      take a long time to calculate.
      We propose a simple and fast translation model learning method
      that directly hallucinates translation rules for infrequent
      phrases from semantically similar neighbors, using
      continuous phrasal representations based on widely available
      word vectors.
      We also investigate approximated nearest neighbors query to
      further speed up search for semantically similar neighbors
      in continuous representation space.
      Experiments show that adding translation rules for infrequent phrases
      generated by our algorithm can improve accuracy by up to 1.5 BLEU.
      Compared to existing methods, our approach generates better
      translation rules, and is faster.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2015
    authors: Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Meg Mitchell, Jianfeng Gao, Bill Dolan, and Jian-Yun Nie
    title: A Neural Network Approach to Context-Sensitive Generation of Conversational Responses
    doc-url: papers/chitchat.pdf
    img: chitchat
    booktitle: Proceedings of NAACL
    booktitle-url: http://naacl.org/naacl-hlt-2015
    venue: conference
    abstract: >
      We present a novel response generation system that can be
      trained from end to end on large quantities of unstructured
      Twitter conversations.
      A neural network architecture is used to address sparsity issues
      that arise when integrating context information into classical
      statistical models, allowing the system to take into account
      previous dialog utterances.
      Our dynamic-context generative models show consistent gains over
      both context-sensitive and non-context-sensitive Machine Translation
      and Information Retrieval baselines.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2014
    authors: Michael Auli, Michel Galley, and Jianfeng Gao
    title: Large Scale Expected BLEU Training of Phrase-based Reordering Models
    doc-url: papers/expbleu-reorder.pdf
    img: expbleu-reorder
    booktitle: Proceedings of EMNLP
    booktitle-url: http://emnlp2014.org/
    venue: conference
    abstract: >
      Recent work by Cherry (2013) has shown that directly
      optimizing phrase-based reordering models towards BLEU
      can lead to significant gains. Their approach is limited
      to small training sets of a few thousand sentences and
      a similar number of sparse features. We show how the
      expected BLEU objective allows us to train a simple
      linear discriminative reordering model with millions
      of sparse features on hundreds of thousands of sentences,
      each of which results in significant improvements.
      A comparison to likelihood training demonstrates that
      expected BLEU is vastly more effective. Our best results
      improve a hierarchical lexicalized reordering baseline
      by up to 2.0 BLEU in a single-reference setting on a
      French-English WMT 2012 setup.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2014
    authors: Michael Auli and Jianfeng Gao
    title: Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models
    doc-url: papers/expbleu-rnn.pdf
    img: expbleu-rnn
    booktitle: Proceedings of ACL
    booktitle-url: http://acl2014.org
    venue: conference
    abstract: >
      Neural network language models are often trained by optimizing likelihood, but
      we would prefer to optimize for a task specific metric, such as BLEU for machine
      translation.
      We show how a recurrent neural network language model can be optimized towards
      an expected BLEU loss instead of the usual cross-entropy criterion.
      Furthermore, we tackle the issue of directly integrating a recurrent
      network into first-pass decoding under an efficient approximation.
      Our best results improve a phrase-based statistical machine translation
      system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the
      expected BLEU objective improves over a cross-entropy trained model by up to
      0.6 BLEU in a single reference setup.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2014
    authors: Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao
    title: Minimum Translation Modeling with Recurrent Neural Networks
    doc-url: papers/mtu-rnn.pdf
    img: mtu-rnn
    booktitle: Proceedings of EACL
    booktitle-url: http://eacl2014.org
    venue: conference
    abstract: >
      We introduce recurrent neural network-based Minimum Translation Unit (MTU)
      models which make predictions based on an unbounded history of previous
      bilingual contexts.
      Traditional back-off n-gram models suffer under the sparse nature of MTUs which
      makes estimation of high-order sequence models challenging.
      We tackle the sparsity problem by modeling MTUs both as bags-of-words and as
      a sequence of individual source and target words.
      Our best results improve the output of a phrase-based statistical machine
      translation system trained on WMT 2012 French-English data by up to 1.5 BLEU,
      and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2013
    authors: Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig
    title: Joint Language and Translation Modeling with Recurrent Neural Networks
    doc-url: papers/rnn-joint-lm-tm.pdf
    img: rnn-joint-lm-tm
    booktitle: Proceedings of EMNLP
    booktitle-url: http://hum.csse.unimelb.edu.au/emnlp2013
    venue: conference
    abstract: >
      We present a joint language and translation model based on a
      recurrent neural network which predicts target words based on
      an unbounded history of both source and target words. The weaker
      independence asssumptions of this model result in a vastly larger
      search space compared to related feed forward-based language or
      translation models. We tackle this issue with a new lattice rescoring
      algorithm and demonstrate its effectiveness empirically. Our joint
      model builds on a well known recurrent neural network language model
      (Mikolov, 2012) augmented by a layer of additional inputs from the
      source language. We show competitive accuracy compared to the
      traditional channel model features. Our best results improve the output
      of a system trained on WMT 2012 French-English data by up to 1.5 BLEU,
      and by 1.1 BLEU on average across several test sets.
  -
    layout: paper
    paper-type: phd-thesis
    selected: y
    year: 2012
    title: Integrated Supertagging and Parsing
    institution: University of Edinburgh
    doc-url: papers/michael.auli.thesis.pdf
    img: thesis2012
    abstract: >
      <p>Parsing is the task of assigning syntactic or semantic structure to a natural language sentence. This thesis focuses on syntactic parsing with Combinatory Categorial Grammar (CCG; Steedman 2000). CCG allows incremental processing, which is essential for speech recognition and some machine translation models, and it can build semantic structure in tandem with syntactic parsing. Supertagging solves a subset of the parsing task by assigning lexical types to words in a sentence using a sequence model. It has emerged as a way to improve the efficiency of full CCG parsing (Clark and Curran, 2007) by reducing the parser's search space. This has been very successful and it is the central theme of this thesis.

      <p>We begin by an analysis of how efficiency is being traded for accuracy in supertagging. Pruning the search space by supertagging is inherently approximate and to contrast this we include A* in our analysis, a classic exact search technique.Interestingly, we find that combining the two methods improves efficiency but we also demonstrate that excessive pruning by a supertagger significantly lowers the upper bound on accuracy of a CCG parser.

      <p>Inspired by this analysis, we design a single integrated model with both supertagging and parsing features, rather than separating them into distinctmodels chained together in a pipeline. To overcome the resulting complexity, we experiment with both loopy belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem.

      <p>Finally, we address training the integrated model. We adopt the idea of optimising directly for a task-specific metric such as is common in other areas like statistical machine translation. We demonstrate how a novel dynamic programming algorithm enables us to optimise for F-measure, our task-specific evaluation metric, and experiment with approximations, which prove to be excellent substitutions.

      <p>Each of the presented methods improves over the state-of-the-art in CCG parsing. Moreover, the improvements are additive, achieving a labelled/unlabelled dependency F-measure on CCGbank of 89.3%/94.0% with gold part-of-speech tags, and 87.2%/92.8% with automatic part-of-speech tags, the best reported results for this task to date. Our techniques are general and we expect them to apply to other parsing problems, including lexicalised tree adjoining grammar and context-free grammar parsing.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: Training a Log-Linear Parser with Loss Functions via Softmax-Margin
    doc-url: papers/softmax-ccg.pdf
    img: softmax-ccg
    booktitle: Proceedings of EMNLP
    booktitle-url: http://conferences.inf.ed.ac.uk/emnlp2011/
    venue: conference
    abstract: >
      Log-linear parsing models are often trained by optimizing
      likelihood, but we would prefer to optimize for a task-specific metric like F-measure.
      Softmax-margin is a convex objective for such models that minimizes a bound on
      expected risk for a given loss function, but its naïve application requires the loss
      to decompose over the predicted structure, which is not true of F-measure.
      We use softmax-margin to optimize a log-linear CCG parser for a variety of loss functions, and
      demonstrate a novel dynamic programming algorithm that enables us to use it with
      F-measure, leading to substantial gains in accuracy on CCGBank.  When we embed our
      loss-trained parser into a larger model that includes supertagging features
      incorporated via belief propagation, we obtain further improvements and achieve
      a labelled/unlabelled dependency
      F-measure of 89.3%/94.0% on gold part-of-speech tags,
      and 87.2%/92.8% on automatic part-of-speech
      tags, the best reported results for this task.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing
    doc-url: papers/lbp-vs-dd.pdf
    img: lbp-vs-dd
    booktitle: Proceedings of ACL
    booktitle-url: http://www.acl2011.org/
    venue: conference
    abstract: >
      Via an oracle experiment, we show that the upper bound on accuracy of a CCG
      parser is significantly lowered when its search space is pruned using a
      supertagger, though the supertagger also prunes many bad parses.  Inspired by
      this analysis, we design a single model with both supertagging and parsing
      features, rather than separating them into distinct models chained together
      in a pipeline.  To overcome the resulting increase in complexity, we
      experiment with both belief propagation and dual decomposition approaches to
      inference, the first empirical comparison of these algorithms that we are
      aware of on a structured natural language processing problem.  On CCGbank we
      achieve a labelled dependency F-measure of 88.8% on gold POS tags, and
      86.7% on automatic part-of-speech tags, the best reported results for this
      task.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2011
    authors: Michael Auli and Adam Lopez
    title: Efficient CCG Parsing&#58; A* versus Adaptive Supertagging
    doc-url: papers/astar-ccg.pdf
    img: astar-ccg
    booktitle: Proceedings of ACL
    booktitle-url: http://www.acl2011.org/
    venue: conference
    abstract: >
      We present a systematic comparison and combination of two orthogonal techniques for efficient
      parsing of Combinatory Categorial Grammar (CCG).  First we consider adaptive supertagging, a
      widely used approximate search technique that prunes most lexical categories from the parser's
      search space using a separate sequence model.  Next we consider several variants on A*, a
      classic exact search technique which to our knowledge has not been applied to more expressive
      grammar formalisms like CCG.  In addition to standard hardware-independent measures of parser
      effort we also present what we believe is the first evaluation of A* parsing on the more
      realistic but more stringent metric of CPU time.  By itself, A* substantially reduces parser
      effort as measured by the number of edges considered during parsing, but we show that for CCG
      this does not always correspond to improvements in CPU time over a CKY baseline.  Combining A*
      with adaptive supertagging decreases CPU time by 15% for our best model.
  -
    layout: paper
    paper-type: firstyearreport
    selected: y
    year: 2009
    institution: University of Edinburgh
    title: CCG-based Models for Statistical Machine Translation
    doc-url: papers/first-year-report.pdf
    img: first-year-report
    abstract: >
      <p>The arguably best performing statistical machine translation systems are based
      on context-free formalisms or weakly equivalent ones. These models usually use
      a synchronous version of a context-free grammar (SCFG) which we argue is too
      rigid for the highly ambiguous task of human language translation. This is
      exacerbated by the fact that the imperfect methods available for aligning
      parallel texts make extracting an efficient grammar very hard. As a result,
      the context-free grammars extracted are usually very large in size after
      having already been restricted through a variety of constraints.

      <p>We propose to use Combinatorial Categorial Grammar (CCG) for machine translation
      models. CCG is a lexicalized, mildly-context-sensitive formalism which
      is very well suited to capture long-distance dependencies that are not
      addressed very well by most current models. We believe that CCG is very well
      suited for the task of machine translation due to its ability to represent
      non-constituents in a syntactic way which frequently occur in parallel texts
      as well as its high derivational flexibility. This allows us to use some of
      the advantages of non-syntactic phrase-based approaches within a syntactic
      framework such as a relatively small grammar size compared to
      context-free-based machine translation grammars.

      <p>A number of models leveraging the advantages of CCG are possible, however, our
      principal goal is to develop a string-to-tree based model which projects CCG
      on the target side of a synchronous grammar. We intend to apply the vast
      progress made in monolingual CCG parsing to machine translation. Additionally,
      we propose to extend CCG to a synchronous grammar (SCCG) as it has been done
      for other related formalisms such as tree adjoining grammars. We hope that a
      SCCG may provide similar derivational flexibility to monolingual CCG which may
      result in a better model for translational equivalence.
  -
    layout: paper
    paper-type: inproceedings
    selected: y
    year: 2009
    authors: Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn
    title: A Systematic Analysis of Translation Model Search Spaces
    doc-url: http://www.aclweb.org/anthology/W/W09/W09-0437.pdf
    booktitle: Proceedings of the Fourth Workshop on Statistical Machine Translation
    booktitle-url: http://www.statmt.org/wmt09/
    img: wmt2009
    venue: workshop
    abstract: >
      Translation systems are complex, and most metrics do little to pinpoint causes of error or isolate system differences.  We use a simple technique to discover induction errors, which occur when good translations are absent from model search spaces.  Our results show that a common pruning heuristic drastically increases induction error, and also strongly suggest that the search spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences.
